{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Future Sales\n",
    "\n",
    "## How to\n",
    "\n",
    "You don't have to do anything special, pretrained models are provided. Simply clone repo, run this notebook and enjoy. \n",
    "\n",
    "Can send result_lgb+lr.csv file for evaluation\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Notebook was created in typical anaconda environment\n",
    "\n",
    "Uncomment and run cell below. to install lightgbm (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import product\n",
    "import gc\n",
    "import tqdm.notebook as tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, PredefinedSplit\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack, vstack\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import time\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "def downcast_dtypes(df):    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def concat_df(train_data, test_data):\n",
    "    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True), train_data.shape[0]\n",
    "\n",
    "def divide_df(all_data, train_size):\n",
    "    if 'target' in all_data:\n",
    "        return all_data.loc[:train_size-1], all_data.loc[train_size:].drop(['target'], axis=1)\n",
    "    return all_data.loc[:train_size-1], all_data.loc[train_size:]\n",
    "\n",
    "def get_result_df(predict):\n",
    "    result = pd.DataFrame(data={'ID': range(0, 214200), 'item_cnt_month': predict})\n",
    "    result['item_cnt_month'] = result['item_cnt_month'].clip(0, 20)\n",
    "    return result\n",
    "\n",
    "def get_mix(alpha, X):\n",
    "    return (alpha * X[:,0]) + ((1-alpha) * X[:,1])\n",
    "\n",
    "def get_best_alpha(X_train_level2, target):\n",
    "    alphas_to_try = np.linspace(0, 1, 1001)\n",
    "    min_mse = 100\n",
    "    best_alpha = 1\n",
    "    \n",
    "    for alpha in alphas_to_try:\n",
    "        mix = get_mix(alpha, X_train_level2)\n",
    "        mse = mean_squared_error(target, mix)\n",
    "        if min_mse > mse:\n",
    "            min_mse = mse\n",
    "            best_alpha = alpha\n",
    "    return best_alpha\n",
    "\n",
    "# this factor is needed for pipeline testing, make < 1 to reduce amount of data in work\n",
    "cv_fraction = 1\n",
    "def get_items_subset(X_train): \n",
    "    item_set = X_train['item_id'].unique()\n",
    "    #random.shuffle(item_set)\n",
    "    l = int(len(item_set) * cv_fraction)\n",
    "    cv_item_set = item_set[:l]\n",
    "    return X_train['item_id'].isin(cv_item_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('sales_train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_shops = pd.read_csv('shops.csv')\n",
    "df_items = pd.read_csv('items.csv')\n",
    "df_item_cats = pd.read_csv('item_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_PRICE_STATS = False\n",
    "enable_subtype_code_mean = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing item_cnt_day outliers\n",
    "def remove_outlier(df_train, col):\n",
    "    Q1 = df_train[col].quantile(0.25)\n",
    "    Q3 = df_train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = (df_train[col] > Q3+1.5*IQR)\n",
    "    df_train[outliers].shape\n",
    "    df_train.drop(df_train[outliers].index, inplace=True)\n",
    "#remove_outlier(df_train, 'item_cnt_day')\n",
    "#remove_outlier(df_train, 'item_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MONTH why not added??? -bad result (not one hot encoded) least for trees\n",
    "\n",
    "# can be good of LR\n",
    "#df_train['month'] = pd.to_datetime(df_train['date'], format='%d.%m.%Y').dt.month\n",
    "# TODO add week, then add mean by week?\n",
    "\n",
    "# FIX SHOPS\n",
    "\n",
    "# Якутск Орджоникидзе, 56\n",
    "df_train.loc[df_train.shop_id == 0, 'shop_id'] = 57\n",
    "df_test.loc[df_test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "df_train.loc[df_train.shop_id == 1, 'shop_id'] = 58\n",
    "df_test.loc[df_test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "df_train.loc[df_train.shop_id == 10, 'shop_id'] = 11\n",
    "df_test.loc[df_test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create igem categories and cities from shops\n",
    "\n",
    "# Extract type and sub type code\n",
    "df_item_cats['split'] = df_item_cats['item_category_name'].str.split('-')\n",
    "df_item_cats['type'] = df_item_cats['split'].map(lambda x: x[0].strip())\n",
    "df_item_cats['type_code'] = LabelEncoder().fit_transform(df_item_cats['type'])\n",
    "# if subtype is nan then type\n",
    "df_item_cats['subtype'] = df_item_cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "df_item_cats['subtype_code'] = LabelEncoder().fit_transform(df_item_cats['subtype'])\n",
    "\n",
    "# features = 10\n",
    "# tfidf = TfidfVectorizer(max_features=features)\n",
    "# df_item_cats['item_category_name'] = df_item_cats['item_category_name'].replace('[0-9!\"\\?\\.)(,\\+\\*\\[\\]/:\\-\\'&]', ' ', regex=True)\n",
    "# txtFeatures = pd.DataFrame(tfidf.fit_transform(df_item_cats['item_category_name']).toarray())\n",
    "# cols = txtFeatures.columns\n",
    "\n",
    "df_item_cats = df_item_cats[['item_category_id','type_code', 'subtype_code']]\n",
    "# for i in range(features):\n",
    "#     df_item_cats['item_category_tfidf_' + str(i)] = txtFeatures[cols[i]]\n",
    "\n",
    "# Extract city\n",
    "df_shops.loc[df_shops['shop_name'] == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "df_shops['city'] = df_shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "df_shops.loc[df_shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "\n",
    "# add distance to Moscow?\n",
    "\n",
    "df_shops['city_code'] = LabelEncoder().fit_transform(df_shops['city'])\n",
    "df_shops = df_shops[['shop_id','city_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Processing text features\n",
    "\n",
    "df_items_txt = df_items[['item_id', 'item_name']]\n",
    "df_items_txt['item_name'] = df_items_txt['item_name'].replace('[0-9!\"\\?\\.)(,\\+\\*\\[\\]/:\\-\\'&]', ' ', regex=True)\n",
    "\n",
    "features = 10\n",
    "tfidf = TfidfVectorizer(max_features=features)\n",
    "df_items_txt['item_name_len'] = df_items_txt['item_name'].map(len)  \n",
    "df_items_txt['item_name_wc'] = df_items_txt['item_name'].map(lambda x: len(str(x).split(' '))) \n",
    "txtFeatures = pd.DataFrame(tfidf.fit_transform(df_items_txt['item_name']).toarray())\n",
    "cols = txtFeatures.columns\n",
    "\n",
    "for i in range(features):\n",
    "    df_items_txt['item_name_tfidf_' + str(i)] = txtFeatures[cols[i]]\n",
    "\n",
    "df_items_txt.drop(columns=['item_name'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare mean encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "def get_all_data(df):    \n",
    "    \n",
    "    sales = df.copy()\n",
    "    sales = pd.merge(sales, df_items[['item_id','item_category_id']], how='left', on='item_id')\n",
    "    sales = pd.merge(sales, df_item_cats, how='left', on='item_category_id')\n",
    "    # Create \"grid\" with columns\n",
    "    \n",
    "    # For every month we create a grid from all shops/items combinations from that month\n",
    "    grid = [] \n",
    "    for block_num in sales['date_block_num'].unique():\n",
    "        cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "        cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))\n",
    "    \n",
    "    # Turn the grid into a dataframe\n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "    \n",
    "    # Groupby data to get shop-item-month aggregates\n",
    "    gb = sales.groupby(index_cols, as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "    gb.rename(columns={'item_cnt_day': 'target'}, inplace=True)\n",
    "    \n",
    "    # Fix column names\n",
    "    # Join it to the grid\n",
    "    all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)#0.334?\n",
    "    \n",
    "    # Same as above but with shop-month aggregates\n",
    "    gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "    gb.rename(columns={'item_cnt_day': 'target_shop'}, inplace=True)\n",
    "    all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "    \n",
    "    # Same as above but with item-month aggregates\n",
    "    gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "    gb.rename(columns={'item_cnt_day': 'target_item'}, inplace=True)\n",
    "    all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)    \n",
    "    \n",
    "    ### Advanced features statistics for price\n",
    "    # mean price\n",
    "    if ENABLE_PRICE_STATS:\n",
    "        gb = sales.groupby(['item_id', 'shop_id', 'date_block_num'],as_index=False).agg({'item_price':'mean'})\n",
    "        gb.columns = ['item_id', 'shop_id', 'date_block_num', 'item_price_mean']\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'shop_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        all_data['sale_total'] = all_data['target'] * all_data['item_price_mean']\n",
    "        \n",
    "    if enable_subtype_code_mean:\n",
    "        # min/max price by 'subtype_code'\n",
    "        all_data = pd.merge(all_data, sales[['item_id','item_category_id']].drop_duplicates(), how='left', on='item_id')\n",
    "        all_data = pd.merge(all_data,  df_item_cats[['item_category_id', 'subtype_code']], how='left', on='item_category_id')\n",
    "        gb = sales.groupby(['subtype_code', 'shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "        gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "        gb.columns = ['subtype_code', 'shop_id', 'date_block_num', 'target_subtype_code']\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['subtype_code', 'shop_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        all_data.drop(columns=['item_category_id', 'subtype_code'], inplace=True)\n",
    "    \n",
    "    df_test_concat = df_test.drop(columns=['ID'])\n",
    "    df_test_concat['date_block_num'] = 34\n",
    "    all_data, TRAIN_SIZE = concat_df(all_data, df_test_concat)\n",
    "    \n",
    "    all_data = downcast_dtypes(all_data)\n",
    "#     del grid, gb, df_test_concat, sales, df_test\n",
    "#     gc.collect();\n",
    "    return all_data, TRAIN_SIZE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare historical lags item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "def get_lags(df):\n",
    "    \n",
    "    # List of columns that we will use to create lags\n",
    "    cols_to_rename = list(df.columns.difference(index_cols)) \n",
    "    \n",
    "    df = df.copy()\n",
    "    for month_shift in tqdm.tqdm(shift_range):\n",
    "        train_shift = df[index_cols + cols_to_rename].copy()\n",
    "        \n",
    "        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "        \n",
    "        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "        train_shift = train_shift.rename(columns=foo)\n",
    "    \n",
    "        df = pd.merge(df, train_shift, on=index_cols, how='left').fillna(0)\n",
    "    \n",
    "    del train_shift\n",
    "    \n",
    "    # try to delete medians\n",
    "    df.drop(columns=['target_shop', 'target_item'], inplace=True)\n",
    "    if ENABLE_PRICE_STATS:\n",
    "        df.drop(columns=['item_price_mean', 'sale_total'], inplace=True)\n",
    "        \n",
    "    if enable_subtype_code_mean:    \n",
    "        df.drop(columns=['target_subtype_code'], inplace=True)\n",
    "    \n",
    "    # List of all lagged features\n",
    "    fit_cols = [col for col in df.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "    # We will drop these at fitting stage\n",
    "    to_drop_cols = list(set(list(df.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "    \n",
    "    # Category for each item\n",
    "    item_category_mapping = df_items[['item_id','item_category_id']].drop_duplicates()\n",
    "    \n",
    "    df = pd.merge(df, item_category_mapping, how='left', on='item_id')\n",
    "    df = pd.merge(df, df_item_cats, how='left', on='item_category_id')\n",
    "    df = pd.merge(df, df_shops, how='left', on='shop_id')\n",
    "    df = pd.merge(df, df_items_txt, how='left', on='item_id')\n",
    "    \n",
    "#     months = df_train.groupby(['date_block_num'], as_index=False)['month'].mean()\n",
    "    \n",
    "#     df = pd.merge(df, months, how='left', on='date_block_num')\n",
    "    \n",
    "#     df['month'] = ((df['date_block_num'] % 12)+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generating categorical interaction feature for experiment/education\n",
    "    # all_data['item_category_id_city_code'] = all_data['item_category_id'].astype(str) + '_' + all_data['city_code'].astype(str)\n",
    "    # all_data['item_category_id_city_code'] = LabelEncoder().fit_transform(all_data['item_category_id_city_code'])\n",
    "    return downcast_dtypes(df)\n",
    "    # del df_items_txt\n",
    "    # gc.collect();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train/validation validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def trim_12(df):\n",
    "    return df[df['date_block_num'] >= 12]\n",
    "\n",
    "# TODO should we have subtype_code as it encodes item_category_id\n",
    "cat_columns = ['item_id', \n",
    "               'shop_id', \n",
    "               'item_category_id', 'type_code', 'subtype_code','city_code']#, \n",
    "               #'item_category_id_city_code]\n",
    "\n",
    "\n",
    "def split_train_and_postprocess_for_linear(X_train, Y_train, X_test):\n",
    "    Y_train = Y_train.reset_index(drop=True)\n",
    "    \n",
    "    df_all_data, train_size = concat_df(X_train, X_test)\n",
    "    \n",
    "    for cat_column in cat_columns:\n",
    "        df_all_data[cat_column] = df_all_data[cat_column].astype('category')\n",
    "             \n",
    "    df_cat = df_all_data[cat_columns + ['date_block_num']]\n",
    "    df_scale = df_all_data[list(set(df_all_data.columns) - set(cat_columns))]\n",
    "      \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_scale)\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(df_cat[cat_columns])\n",
    "    \n",
    "    train_cat, X_test_cat = divide_df(df_cat, train_size)\n",
    "    X_test_cat = X_test_cat.drop(columns=['date_block_num'])\n",
    "    \n",
    "    max_date = train_cat['date_block_num'].max()\n",
    "               \n",
    "    cv_X_test_cat = train_cat[train_cat['date_block_num'] == max_date].drop(columns=['date_block_num'])\n",
    "    cv_X_train_cat = train_cat[train_cat['date_block_num'] < max_date].drop(columns=['date_block_num'])\n",
    "    \n",
    "    train_scale, X_test_scale = divide_df(df_scale, train_size)\n",
    "    cv_X_test_scale = train_scale[train_scale['date_block_num'] == max_date]\n",
    "    cv_Y_test = Y_train[train_scale['date_block_num'] == max_date]\n",
    "    \n",
    "    cv_Y_train = Y_train[train_scale['date_block_num'] < max_date]\n",
    "    cv_X_train_scale = train_scale[train_scale['date_block_num'] < max_date]\n",
    "               \n",
    "    # scaling\n",
    "    X_test_scale = scaler.transform(X_test_scale)\n",
    "    cv_X_test_scale = scaler.transform(cv_X_test_scale)\n",
    "    cv_X_train_scale = scaler.transform(cv_X_train_scale)\n",
    "    \n",
    "    X_test_cat = encoder.transform(X_test_cat)\n",
    "    X_test_scale = csr_matrix(X_test_scale)\n",
    "    X_test = hstack((X_test_cat, X_test_scale))\n",
    "               \n",
    "    cv_X_test_cat = encoder.transform(cv_X_test_cat)\n",
    "    cv_X_test_scale = csr_matrix(cv_X_test_scale)\n",
    "    cv_X_test = hstack((cv_X_test_cat, cv_X_test_scale))\n",
    "    \n",
    "    cv_X_train_cat = encoder.transform(cv_X_train_cat)    \n",
    "    cv_X_train_scale = csr_matrix(cv_X_train_scale)\n",
    "    cv_X_train = hstack((cv_X_train_cat, cv_X_train_scale))\n",
    "               \n",
    "    return cv_X_train, cv_Y_train, cv_X_test, cv_Y_test,X_test\n",
    "\n",
    "def preprocess_for_linear(X_train, X_test):\n",
    "    df_all_data, train_size = concat_df(X_train, X_test)\n",
    "    for cat_column in cat_columns:\n",
    "        df_all_data[cat_column] = df_all_data[cat_column].astype('category')\n",
    "            \n",
    "    df_cat = df_all_data[cat_columns]\n",
    "    df_scale = df_all_data[list(set(df_all_data.columns) - set(cat_columns))]\n",
    "      \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_scale)\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(df_cat)\n",
    "    \n",
    "    train_cat, X_test_cat = divide_df(df_cat, train_size)\n",
    "    train_scale, X_test_scale = divide_df(df_scale, train_size)\n",
    "               \n",
    "    # scaling\n",
    "    X_test_scale = scaler.transform(X_test_scale)\n",
    "    X_test_cat = encoder.transform(X_test_cat)\n",
    "    X_test = hstack((X_test_cat, X_test_scale))\n",
    "    \n",
    "    train_cat = encoder.transform(train_cat)\n",
    "    train_scale = scaler.transform(train_scale)\n",
    "    X_train = hstack((train_cat, train_scale))\n",
    "               \n",
    "    return X_train, X_test\n",
    "               \n",
    "def train_and_predict_lr(X, Y, X_test, model_name):\n",
    "    pkl_filename = f'{model_name}.pkl'\n",
    "    if pathlib.Path(pkl_filename).exists():\n",
    "        print(f'Reading model {pkl_filename} from file')\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            lr = pickle.load(file)\n",
    "    else:\n",
    "        lr = SGDRegressor()\n",
    "        lr.fit(X, Y)\n",
    "        print(f'Storing model {pkl_filename} in file')\n",
    "        #with open(pkl_filename, 'wb') as file:\n",
    "            #pickle.dump(lr, file)\n",
    "    pred_lr = lr.predict(X_test)\n",
    "    return pred_lr\n",
    "\n",
    "def tune_lr_hp(X_tr, Y_tr, X_ts, Y_ts, model='SGD'):\n",
    "    \n",
    "    pkl_filename = f'models/{model}.pkl'\n",
    "    if pathlib.Path(pkl_filename).exists():\n",
    "        print(f'Reading model from file {pkl_filename}')\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "    \n",
    "    X = vstack((X_tr, X_ts))\n",
    "    Y = pd.concat([Y_tr, Y_ts])    \n",
    "    print(f'X: {X.shape}, Y: {Y.shape}')\n",
    "    \n",
    "    def cv_generator():        \n",
    "        train_ind = list(range(0, X_tr.shape[0]))\n",
    "        test_ind = list(range(X_tr.shape[0], X_tr.shape[0] + X_ts.shape[0]))\n",
    "        \n",
    "        print(f'X_tr shape: {X_tr.shape}, X_ts shape: {X_ts.shape}')\n",
    "        print(f'train_ind=[{train_ind[0]}, {train_ind[-1]}], test_ind=[{test_ind[0]}, {test_ind[-1]}]')\n",
    "        yield train_ind, test_ind\n",
    "    \n",
    "    n_HP_points_to_test = 10\n",
    "    \n",
    "    params = {'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "              'l1_ratio': sp_uniform(loc=0.05, scale=0.9),\n",
    "              'alpha' : 10.0**-np.arange(1,7),\n",
    "              'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive']}\n",
    "    fit_params={}\n",
    "    \n",
    "    gs = RandomizedSearchCV(\n",
    "            estimator=SGDRegressor(), param_distributions=params, \n",
    "            n_iter=n_HP_points_to_test,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            cv=cv_generator(),\n",
    "            refit=True,\n",
    "            random_state=314,\n",
    "            verbose=False, n_jobs=2)\n",
    "    gs.fit(X, Y, **fit_params)\n",
    "    pd.DataFrame.from_dict(gs.cv_results_).to_csv('lr_cv.csv')\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(gs.best_estimator_, file)\n",
    "    return gs.best_estimator_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train lightgbm model with HP optimization and prediction\n",
    "to speedup review we are storing optimized hyperparams, model training should not take much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(all_data):\n",
    "    X = X.copy()\n",
    "    X['target'] = Y\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    print('X.index')\n",
    "    print(X.index)\n",
    "    for train_index, val_index in skf.split(X, X[regularize_column]):\n",
    "        df_train, df_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        group = df_train.groupby(regularize_column).target.mean()\n",
    "        X.loc[val_index, target_enc] = X.loc[val_index, regularize_column].map(group)\n",
    "    X[target_enc].fillna(0.3343, inplace=True)\n",
    "    return X[target_enc]\n",
    "\n",
    "def get_cv_time_series_split(all_data, n_splits=4):\n",
    "    min_date = all_data['date_block_num'].min()\n",
    "    max_date = all_data['date_block_num'].max()\n",
    "    print(f'min_date: {min_date}, max_date: {max_date}')\n",
    "    months = random.sample(range(min_date+9, max_date), n_splits)\n",
    "    for month in months:        \n",
    "        train_index = all_data[(all_data['date_block_num'] < month)].index\n",
    "        test_index = all_data[all_data['date_block_num'] == month].index\n",
    "        print(f'time series cv split month: {month}, train size: {len(train_index)}, test size: {len(test_index)}')\n",
    "        yield train_index.values, test_index.values\n",
    "\n",
    "def optimize_lgb_parameters(X, Y, X_val, Y_val, month=''):\n",
    "    params_file = f'models/lgb_parameters_{month}.json'\n",
    "    if pathlib.Path(params_file).exists():\n",
    "        with open(params_file) as f:\n",
    "            params = json.load(f)\n",
    "            print('Lgbm parameters read from file.')\n",
    "            print(params)\n",
    "        return params\n",
    "    else:\n",
    "        print('Lgbm HP optimization...')\n",
    "#         lgb_params ={'num_leaves': sp_randint(6, 50), \n",
    "#              'min_child_samples': sp_randint(100, 500), \n",
    "#              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "#              'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "#              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "#              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "#              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
    "#              'bagging_fraction': sp_uniform(loc=0.5, scale=0.45),\n",
    "#              'feature_fraction': sp_uniform(loc=0.5, scale=0.45),\n",
    "#              'min_data_in_leaf': sp_randint(0, 300),\n",
    "# #                 'learning_rate': [0.03]\n",
    "#                     }\n",
    "        lgb_params ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "        fit_params={\"early_stopping_rounds\":30, \n",
    "                    \"eval_metric\" : 'rmse', \n",
    "                    \"eval_set\" : [(X_val, Y_val)],\n",
    "                    'eval_names': ['valid'],\n",
    "                    'verbose': 100,\n",
    "#                     'categorical_feature': 'auto'\n",
    "                   }\n",
    "        \n",
    "        n_HP_points_to_test = 50\n",
    "        \n",
    "        clf = lgb.LGBMRegressor(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=-1, \n",
    "                                n_estimators=50)\n",
    "        X = X.reset_index(drop=True)\n",
    "        Y = Y.reset_index(drop=True)\n",
    "        cv_generator = get_cv_time_series_split(X)\n",
    "        gs = RandomizedSearchCV(\n",
    "            estimator=clf, param_distributions=lgb_params, \n",
    "            n_iter=n_HP_points_to_test,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            cv=cv_generator,\n",
    "            refit=True,\n",
    "            random_state=314,\n",
    "            verbose=False)\n",
    "        gs.fit(X, Y, **fit_params)\n",
    "        print(f'Best score reached: {gs.best_score_} with params: {gs.best_params_}')\n",
    "        print(f'Feature importance HP optimization: {gs.best_estimator_.feature_importances_}')\n",
    "#         print(f'Model: {gs.best_estimator_.model_to_string()}')\n",
    "        params = gs.best_params_\n",
    "        with open(params_file, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "        pd.DataFrame.from_dict(gs.cv_results_).to_csv(f'results/lgb_csv_{month}.csv')\n",
    "        return params\n",
    "\n",
    "def train_and_predict_lgbm(X, Y, X_test, params, month=''):\n",
    "    model_file = f'models/lgb_{month}.txt'\n",
    "    if pathlib.Path(model_file).exists():\n",
    "        print(f'reading lgb model from {model_file}')\n",
    "        model = lgb.Booster(model_file=model_file)\n",
    "    else:\n",
    "        print(f'training for {model_file} by parameters')\n",
    "        model = lgb.train(params, lgb.Dataset(X, label=Y), 100)\n",
    "        model.save_model(model_file)\n",
    "        \n",
    "    pred_lgb = model.predict(X_test)\n",
    "    return pred_lgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d883033732a7409a846acc1926ddbcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_data, TRAIN_SIZE = get_all_data(df_train)\n",
    "all_data = get_lags(all_data)\n",
    "\n",
    "# ???\n",
    "# for cat_column in cat_columns:\n",
    "#     all_data[cat_column] = all_data[cat_column].astype('category')\n",
    "\n",
    "train, X_test = divide_df(all_data, TRAIN_SIZE)\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "train = trim_12(train) \n",
    "\n",
    "item_ids_subset = get_items_subset(train)\n",
    "train = train[item_ids_subset]\n",
    "\n",
    "X_train = train.drop(columns=['target'])\n",
    "Y_train = train['target']\n",
    "\n",
    "max_date = X_train['date_block_num'].max()\n",
    "\n",
    "# IMPORTANT this is holdout validation test set\n",
    "cv_X_test = X_train[X_train['date_block_num'] == max_date]\n",
    "cv_Y_test = Y_train[X_train['date_block_num'] == max_date]\n",
    "\n",
    "cv_Y_train = Y_train[X_train['date_block_num'] < max_date]\n",
    "cv_X_train = X_train[X_train['date_block_num'] < max_date]\n",
    "\n",
    "del df_train, all_data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_result = {}\n",
    "def get_meta_featues_train(X_tr, Y_tr, X_ts):\n",
    "\n",
    "    dates_train = X_tr['date_block_num']\n",
    "    \n",
    "    meta_dates = [27, 28, 29, 30, 31, 32, 33]\n",
    "\n",
    "#     meta_dates = [27]\n",
    "    \n",
    "    dates_train_level2 = dates_train[dates_train.isin(meta_dates)]\n",
    "    \n",
    "    # That is how we get target for the 2nd level dataset\n",
    "    y_train_level2 = Y_tr[dates_train.isin(meta_dates)]\n",
    "    \n",
    "    # And here we create 2nd level feeature matrix, init it with zeros first\n",
    "    X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "    \n",
    "    # Fit N diverse models on those M chunks and predict for the chunk M+1. \n",
    "    # Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end.\n",
    "    for cur_block_num in meta_dates:\n",
    "        print(f'Ensembling month {cur_block_num}')\n",
    "        \n",
    "        '''\n",
    "            1. Split `X_train` into parts\n",
    "               Remember, that corresponding dates are stored in `dates_train` \n",
    "            2. Fit linear regression \n",
    "            3. Fit LightGBM and put predictions          \n",
    "            4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "               You can use `dates_train_level2` for it\n",
    "               Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "        '''      \n",
    "        \n",
    "        train_x = X_tr.loc[dates_train < cur_block_num]\n",
    "        train_y = Y_tr[dates_train < cur_block_num]\n",
    "        \n",
    "        test_x = X_tr.loc[dates_train == cur_block_num]\n",
    "        test_y = Y_tr[dates_train == cur_block_num]\n",
    "        \n",
    "        cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr, X_test_lr \\\n",
    "            = split_train_and_postprocess_for_linear(train_x, train_y, test_x)\n",
    "        \n",
    "        be = tune_lr_hp(cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr, model=f'lr_meta_{cur_block_num}')\n",
    "        pred_lr = be.predict(X_test_lr)\n",
    "        lr_rmse = mean_squared_error(test_y, pred_lr, squared=False)\n",
    "        meta_result[f'lr_rmse_{cur_block_num}'] = [lr_rmse]\n",
    "        \n",
    "        cv_max_date = train_x['date_block_num'].max()\n",
    "\n",
    "        # IMPORTANT this is holdout validation test set\n",
    "        cv_X_ts = train_x[train_x['date_block_num'] == cv_max_date]\n",
    "        cv_Y_ts = train_y[train_x['date_block_num'] == cv_max_date]\n",
    "        \n",
    "        cv_Y_tr = train_y[train_x['date_block_num'] < cv_max_date]\n",
    "        cv_X_tr = train_x[train_x['date_block_num'] < cv_max_date]        \n",
    "        \n",
    "        lgb_parameters = optimize_lgb_parameters(cv_X_tr, cv_Y_tr, cv_X_ts, cv_Y_ts, month=cur_block_num)\n",
    "        pred_lgb = train_and_predict_lgbm(train_x, train_y, test_x, lgb_parameters, month=cur_block_num)\n",
    "        lgb_rmse = mean_squared_error(test_y, pred_lgb, squared=False)\n",
    "        meta_result[f'lgb_rmse_{cur_block_num}'] = [lgb_rmse]\n",
    "                \n",
    "        X_train_level2[dates_train_level2==cur_block_num, 0] = pred_lr\n",
    "        X_train_level2[dates_train_level2==cur_block_num, 1] = pred_lgb\n",
    "        \n",
    "    \n",
    "    best_alpha = get_best_alpha(X_train_level2, y_train_level2)\n",
    "    meta_result['best_alpha'] = [best_alpha]\n",
    "    test_preds = get_mix(best_alpha, np.c_[pred_lr, pred_lgb])\n",
    "    rmse_mix = mean_squared_error(test_y, test_preds)\n",
    "    meta_result['rmse_mix'] = [rmse_mix]\n",
    "    \n",
    "    # Use all train data to fit models and get predictions for test.\n",
    "    be.fit(X_tr, Y_tr)\n",
    "    pred_lr = be.predict(X_ts)\n",
    "    pred_lgb = train_and_predict_lgbm(X_tr, Y_tr, X_ts, lgb_parameters, 33)\n",
    "    X_test_level2 = np.c_[pred_lr, pred_lgb] \n",
    "    \n",
    "    return X_train_level2, y_train_level2, X_test_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembling month 27\n",
      "X: (4810730, 15494), Y: (4810730,)\n",
      "X_tr shape: (4530958, 15494), X_ts shape: (279772, 15494)\n",
      "train_ind=[0, 4530957], test_ind=[4530958, 4810729]\n",
      "Lgbm HP optimization...\n",
      "min_date: 12, max_date: 25\n",
      "time series cv split month: 24, train size: 3939517, test size: 306950\n",
      "time series cv split month: 22, train size: 3293167, test size: 316100\n",
      "time series cv split month: 21, train size: 2963799, test size: 329368\n",
      "time series cv split month: 23, train size: 3609267, test size: 330250\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.81784\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.802993\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.804922\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.796362\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.819007\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.796726\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.808086\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.793461\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.844026\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.816561\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.822388\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.818266\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.81881\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.795595\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.79794\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.791559\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.818554\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.798028\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.798874\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.796747\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.83454\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.813608\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid's rmse: 0.815482\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.81244\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.817947\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[40]\tvalid's rmse: 0.79716\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.801838\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.793345\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.814561\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.796057\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.798799\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.796822\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.832538\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.811093\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.813228\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.808589\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.820097\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.80375\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.809589\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.795302\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.814454\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.795249\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.797518\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.786316\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.820077\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.794481\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.801724\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.790474\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.818025\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.794396\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[45]\tvalid's rmse: 0.803225\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.79384\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.84573\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.821086\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.825534\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.821183\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid's rmse: 0.830393\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.810688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.810521\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.806473\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.814641\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.795634\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.796468\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.788136\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.832334\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.812119\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.818773\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.81035\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.825042\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.805602\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.808571\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.80016\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.812345\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[45]\tvalid's rmse: 0.794838\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.799537\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.789568\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.819153\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.802303\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.807989\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid's rmse: 0.78805\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.832306\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.814403\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[47]\tvalid's rmse: 0.815489\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.809421\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.835955\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.81395\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.814036\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.814465\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid's rmse: 0.822926\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.805372\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.80696\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.797384\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.831116\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.815804\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.816555\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.811718\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.817451\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.797106\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.802811\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.787909\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.843508\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.818577\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.823062\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.819996\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.826381\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.804845\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.804455\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.796516\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.813451\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.798564\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.801059\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.78389\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid's rmse: 0.830854\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.815729\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.817815\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.811886\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.821638\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.794514\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.798362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[39]\tvalid's rmse: 0.7925\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid's rmse: 0.817527\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid's rmse: 0.800661\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.80554\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[44]\tvalid's rmse: 0.793345\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.816622\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[38]\tvalid's rmse: 0.798965\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.798384\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.790109\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid's rmse: 0.824524\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.802602\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[45]\tvalid's rmse: 0.806941\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid's rmse: 0.79609\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.843614\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.81707\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid's rmse: 0.821695\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.819274\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.823458\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.802337\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.80282\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.79582\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.818414\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.800539\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid's rmse: 0.80509\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.793768\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.821805\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.801539\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.803302\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.795638\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid's rmse: 0.842309\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.817001\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.821881\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[43]\tvalid's rmse: 0.81835\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.816557\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid's rmse: 0.795041\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.797516\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[37]\tvalid's rmse: 0.791477\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.810308\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.791158\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.793758\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.786721\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.820908\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.802679\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.803144\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[43]\tvalid's rmse: 0.791183\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid's rmse: 0.841659\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.816603\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[45]\tvalid's rmse: 0.823034\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.817529\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.8292\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.80459\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.805706\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid's rmse: 0.799466\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.81929\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.802587\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.808143\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.795436\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid's rmse: 0.836582\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.808602\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.812456\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.805033\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.816149\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.79546\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[39]\tvalid's rmse: 0.803886\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.795219\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.821513\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.799884\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.802741\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.796911\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.828879\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[47]\tvalid's rmse: 0.80732\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.808646\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.805848\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.816438\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.79446\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.794329\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.785716\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.841616\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.81765\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.8233\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.817461\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.784251\n",
      "Best score reached: -1.0231061368990901 with params: {'colsample_bytree': 0.46716991724263107, 'min_child_samples': 439, 'min_child_weight': 1, 'num_leaves': 46, 'reg_alpha': 1, 'reg_lambda': 50, 'subsample': 0.20651472131008397}\n",
      "Feature importance HP optimization: [170 138 154 130  57  24  71  61  53 106  36  15  47  16  32  53  14  31\n",
      "  40  32  25 165  70 128 121 147  91  16   0   0   7  34  37  33   8  58\n",
      "  30]\n",
      "training for models/lgb_27.txt by parameters\n",
      "Ensembling month 28\n",
      "X: (5068102, 15736), Y: (5068102,)\n",
      "X_tr shape: (4810730, 15736), X_ts shape: (257372, 15736)\n",
      "train_ind=[0, 4810729], test_ind=[4810730, 5068101]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 150, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 247, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 240, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 477, in dump\n    return Pickler.dump(self, obj)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 437, in dump\n    self.save(obj)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 890, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 638, in save_reduce\n    save(args)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 789, in save_tuple\n    save(element)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 819, in save_list\n    self._batch_appends(obj)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 846, in _batch_appends\n    save(tmp[0])\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 774, in save_tuple\n    save(element)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 774, in save_tuple\n    save(element)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 789, in save_tuple\n    save(element)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 819, in save_list\n    self._batch_appends(obj)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 846, in _batch_appends\n    save(tmp[0])\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 638, in save_reduce\n    save(args)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 774, in save_tuple\n    save(element)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 510, in save\n    rv = reduce(obj)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\_memmapping_reducer.py\", line 442, in __call__\n    for dumped_filename in dump(a, filename):\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\numpy_pickle.py\", line 480, in dump\n    NumpyPickler(f, protocol=protocol).dump(value)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\pickle.py\", line 437, in dump\n    self.save(obj)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\numpy_pickle.py\", line 279, in save\n    wrapper.write_array(obj, self)\n  File \"C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\numpy_pickle.py\", line 103, in write_array\n    pickler.file_handle.write(chunk.tobytes('C'))\nOSError: [Errno 28] No space left on device\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-97411ef0157c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_level2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_level2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_level2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_meta_featues_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_level2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_level2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_level2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2f8ff7c768b9>\u001b[0m in \u001b[0;36mget_meta_featues_train\u001b[1;34m(X_tr, Y_tr, X_ts)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;33m=\u001b[0m \u001b[0msplit_train_and_postprocess_for_linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mbe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtune_lr_hp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_X_train_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_Y_train_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_X_test_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_Y_test_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'lr_meta_{cur_block_num}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mpred_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mlr_rmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-3351a2a66db7>\u001b[0m in \u001b[0;36mtune_lr_hp\u001b[1;34m(X_tr, Y_tr, X_ts, Y_ts, model)\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m314\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             verbose=False, n_jobs=3)\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lr_cv.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkl_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1529\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "X_train_level2, y_train_level2, X_test_level2 = get_meta_featues_train(X_train, Y_train, X_test)\n",
    "del train\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_level2, y_train_level2)\n",
    "train_preds = lr.predict(X_train_level2)\n",
    "rmse_train_stacking = mean_squared_error(y_train_level2, train_preds)\n",
    "meta_result['rmse_train_stacking'] = [rmse_train_stacking]\n",
    "meta_result['cv_fraction'] = [cv_fraction]\n",
    "\n",
    "test_preds = lr.predict(X_test_level2)\n",
    "get_result_df(test_preds).to_csv('result_ens.csv', index=False)\n",
    "\n",
    "if pathlib.Path('results/meta_features_results.csv').exists():\n",
    "    metrics = pd.read_csv('results/meta_features_results.csv')\n",
    "else:\n",
    "    metrics = pd.DataFrame()\n",
    "metrics = pd.concat([metrics, pd.DataFrame(data={k: [round(v[0], 5)] for k,v in meta_result.items()})], ignore_index=True)\n",
    "metrics.to_csv('results/meta_features_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final train/validation/prediction setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# ROUND I \n",
    "# FINDING OPTIMAL PARAMETERS AND PARAMETER ALPHA FOR MIXING UP RESULTS OF LBG AND LINEAR MODELS\n",
    "\n",
    "lgb_parameters = optimize_lgb_parameters(cv_X_train, cv_Y_train, cv_X_test, cv_Y_test)\n",
    "\n",
    "# Use holdout validation test set to find optimal alpha\n",
    "\n",
    "pred_tree = train_and_predict_lgbm(cv_X_train, cv_Y_train, cv_X_test, lgb_parameters, 'lgb_model_validation')\n",
    "lgb_r2 = r2_score(cv_Y_test, pred_tree)\n",
    "lgb_rmse = mean_squared_error(cv_Y_test, pred_tree, squared=False)\n",
    "\n",
    "cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr, X_test_lr \\\n",
    "    = split_train_and_postprocess_for_linear(X_train, Y_train, X_test)\n",
    "\n",
    "# pred_lr = train_and_predict_lr(cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, 'lr_model_full')\n",
    "be = tune_lr_hp(cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr)\n",
    "pred_lr = be.predict(cv_X_test_lr)\n",
    "lr_r2 = r2_score(cv_Y_test_lr, pred_lr)\n",
    "lr_rmse = mean_squared_error(cv_Y_test_lr, pred_lr, squared=False)\n",
    "\n",
    "X_train_level2 = np.c_[pred_tree, pred_lr]\n",
    "\n",
    "best_alpha = get_best_alpha(X_train_level2, cv_Y_test)\n",
    "mix = get_mix(best_alpha, X_train_level2)\n",
    "mix_r2 = r2_score(cv_Y_test, mix)\n",
    "mix_rmse = mean_squared_error(cv_Y_test, mix, squared=False)\n",
    "\n",
    "results = {'cv_fraction':cv_fraction, \n",
    "           'lr_rmse': lr_rmse, 'lr_r2': lr_r2, \n",
    "           'lgb_rmse': lgb_rmse, 'lgb_r2': lgb_r2, \n",
    "           'mix_rmse': mix_rmse, 'mix_r2': mix_r2, \n",
    "           'alfa': best_alpha}\n",
    "print(results)\n",
    "del cv_X_train, cv_Y_train, cv_X_test, cv_Y_test, cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr,\\\n",
    "    pred_lr, pred_tree\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store and display validation metruics results\n",
    "results_metrics = 'results_metrics.csv'\n",
    "columns=['lr_rmse', 'lr_r2', 'lgb_rmse', 'lgb_r2', 'mix_rmse', 'mix_r2', 'alfa']\n",
    "\n",
    "if pathlib.Path(results_metrics).exists():\n",
    "    metrics = pd.read_csv(results_metrics)\n",
    "else:\n",
    "    metrics = pd.DataFrame(columns=['cv_fraction'] + columns)\n",
    "\n",
    "metrics = pd.concat([metrics, pd.DataFrame(results, index=[len(metrics)+1])])   \n",
    "metrics.to_csv(results_metrics, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n",
    "for cv_fraction in metrics['cv_fraction'].unique():\n",
    "    ax =plt.axes([0, 0, 2, 2])\n",
    "    for col in columns:\n",
    "        sub_metrix = metrics[metrics['cv_fraction'] == cv_fraction]\n",
    "        ax.plot(sub_metrix.index, sub_metrix[col], marker='.', linestyle='-', ms=12, label=col)\n",
    "        for i,j in zip(sub_metrix.index,metrics[col]):\n",
    "            ax.annotate(str(round(j,4)),xy=(i,j))\n",
    "        plt.xlabel(f'cv_fraction: {cv_fraction}')\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ROUND II\n",
    "# TRAIN MODELS ON COMPLETE DATA SET AND GETTING PREDICTIONS FOR SUBMITING\n",
    "X_train_lr, X_test_lr = preprocess_for_linear(X_train, X_test)\n",
    "# pred_lr = train_and_predict_lr(X_train_lr, Y_train, X_test_lr, 'lr_model_full')\n",
    "be.fit(X_train_lr, Y_train)\n",
    "pred_lr = be.predict(X_test_lr)\n",
    "\n",
    "del X_train_lr, X_test_lr\n",
    "gc.collect();\n",
    "\n",
    "pred_tree = train_and_predict_lgbm(X_train, Y_train, X_test, lgb_parameters, 'lgb_model_train')\n",
    "\n",
    "X_train_level2 = np.c_[pred_tree, pred_lr]\n",
    "\n",
    "result = get_mix(best_alpha, X_train_level2)\n",
    "\n",
    "get_result_df(result).to_csv('result_lgb+lr.csv', index=False)\n",
    "# Your public and private LB scores are: 0.962927 and 0.963253.\n",
    "get_result_df(pred_tree).to_csv('result_lgb.csv', index=False)\n",
    "print(\"DONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(time.time() - start)/60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
