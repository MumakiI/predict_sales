{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Future Sales\n",
    "\n",
    "## How to\n",
    "\n",
    "You don't have to do anything special, pretrained models are provided. Simply clone repo, run this notebook and enjoy. \n",
    "\n",
    "Can send result_lgb+lr.csv file for evaluation\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Notebook was created in typical anaconda environment\n",
    "\n",
    "Uncomment and run cell below. to install lightgbm (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import product\n",
    "import gc\n",
    "import tqdm.notebook as tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, PredefinedSplit\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack, vstack\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import time\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "def downcast_dtypes(df):    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def concat_df(train_data, test_data):\n",
    "    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True), train_data.shape[0]\n",
    "\n",
    "def divide_df(all_data, train_size):\n",
    "    if 'target' in all_data:\n",
    "        return all_data.loc[:train_size-1], all_data.loc[train_size:].drop(['target'], axis=1)\n",
    "    return all_data.loc[:train_size-1], all_data.loc[train_size:]\n",
    "\n",
    "def get_result_df(predict):\n",
    "    result = pd.DataFrame(data={'ID': range(0, 214200), 'item_cnt_month': predict})\n",
    "    result['item_cnt_month'] = result['item_cnt_month'].clip(0, 20)\n",
    "    return result\n",
    "\n",
    "def get_mix(alpha, X):\n",
    "    return (alpha * X[:,0]) + ((1-alpha) * X[:,1])\n",
    "\n",
    "def get_best_alpha(X_train_level2, target):\n",
    "    alphas_to_try = np.linspace(0, 1, 1001)\n",
    "    min_mse = 100\n",
    "    best_alpha = 1\n",
    "    \n",
    "    for alpha in alphas_to_try:\n",
    "        mix = get_mix(alpha, X_train_level2)\n",
    "        mse = mean_squared_error(target, mix)\n",
    "        if min_mse > mse:\n",
    "            min_mse = mse\n",
    "            best_alpha = alpha\n",
    "    return best_alpha\n",
    "\n",
    "# this factor is needed for pipeline testing, make < 1 to reduce amount of data in work\n",
    "cv_fraction = 1\n",
    "def get_items_subset(X_train): \n",
    "    item_set = X_train['item_id'].unique()\n",
    "    #random.shuffle(item_set)\n",
    "    l = int(len(item_set) * cv_fraction)\n",
    "    cv_item_set = item_set[:l]\n",
    "    return X_train['item_id'].isin(cv_item_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('sales_train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_shops = pd.read_csv('shops.csv')\n",
    "df_items = pd.read_csv('items.csv')\n",
    "df_item_cats = pd.read_csv('item_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_PRICE_STATS = False\n",
    "enable_subtype_code_mean = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing item_cnt_day outliers\n",
    "def remove_outlier(df_train, col):\n",
    "    Q1 = df_train[col].quantile(0.25)\n",
    "    Q3 = df_train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = (df_train[col] > Q3+1.5*IQR)\n",
    "    df_train[outliers].shape\n",
    "    df_train.drop(df_train[outliers].index, inplace=True)\n",
    "#remove_outlier(df_train, 'item_cnt_day')\n",
    "#remove_outlier(df_train, 'item_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MONTH why not added??? -bad result (not one hot encoded) least for trees\n",
    "\n",
    "# can be good of LR\n",
    "#df_train['month'] = pd.to_datetime(df_train['date'], format='%d.%m.%Y').dt.month\n",
    "# TODO add week, then add mean by week?\n",
    "\n",
    "# FIX SHOPS\n",
    "\n",
    "# Якутск Орджоникидзе, 56\n",
    "df_train.loc[df_train.shop_id == 0, 'shop_id'] = 57\n",
    "df_test.loc[df_test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "df_train.loc[df_train.shop_id == 1, 'shop_id'] = 58\n",
    "df_test.loc[df_test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "df_train.loc[df_train.shop_id == 10, 'shop_id'] = 11\n",
    "df_test.loc[df_test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create igem categories and cities from shops\n",
    "\n",
    "# Extract type and sub type code\n",
    "df_item_cats['split'] = df_item_cats['item_category_name'].str.split('-')\n",
    "df_item_cats['type'] = df_item_cats['split'].map(lambda x: x[0].strip())\n",
    "df_item_cats['type_code'] = LabelEncoder().fit_transform(df_item_cats['type'])\n",
    "# if subtype is nan then type\n",
    "df_item_cats['subtype'] = df_item_cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "df_item_cats['subtype_code'] = LabelEncoder().fit_transform(df_item_cats['subtype'])\n",
    "\n",
    "# features = 10\n",
    "# tfidf = TfidfVectorizer(max_features=features)\n",
    "# df_item_cats['item_category_name'] = df_item_cats['item_category_name'].replace('[0-9!\"\\?\\.)(,\\+\\*\\[\\]/:\\-\\'&]', ' ', regex=True)\n",
    "# txtFeatures = pd.DataFrame(tfidf.fit_transform(df_item_cats['item_category_name']).toarray())\n",
    "# cols = txtFeatures.columns\n",
    "\n",
    "df_item_cats = df_item_cats[['item_category_id','type_code', 'subtype_code']]\n",
    "# for i in range(features):\n",
    "#     df_item_cats['item_category_tfidf_' + str(i)] = txtFeatures[cols[i]]\n",
    "\n",
    "# Extract city\n",
    "df_shops.loc[df_shops['shop_name'] == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "df_shops['city'] = df_shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "df_shops.loc[df_shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "\n",
    "# add distance to Moscow?\n",
    "\n",
    "df_shops['city_code'] = LabelEncoder().fit_transform(df_shops['city'])\n",
    "df_shops = df_shops[['shop_id','city_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aimar\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Processing text features\n",
    "\n",
    "df_items_txt = df_items[['item_id', 'item_name']]\n",
    "df_items_txt['item_name'] = df_items_txt['item_name'].replace('[0-9!\"\\?\\.)(,\\+\\*\\[\\]/:\\-\\'&]', ' ', regex=True)\n",
    "\n",
    "features = 10\n",
    "tfidf = TfidfVectorizer(max_features=features)\n",
    "df_items_txt['item_name_len'] = df_items_txt['item_name'].map(len)  \n",
    "df_items_txt['item_name_wc'] = df_items_txt['item_name'].map(lambda x: len(str(x).split(' '))) \n",
    "txtFeatures = pd.DataFrame(tfidf.fit_transform(df_items_txt['item_name']).toarray())\n",
    "cols = txtFeatures.columns\n",
    "\n",
    "for i in range(features):\n",
    "    df_items_txt['item_name_tfidf_' + str(i)] = txtFeatures[cols[i]]\n",
    "\n",
    "df_items_txt.drop(columns=['item_name'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare mean encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "def get_all_data(df):    \n",
    "    \n",
    "    sales = df.copy()\n",
    "    sales = pd.merge(sales, df_items[['item_id','item_category_id']], how='left', on='item_id')\n",
    "    sales = pd.merge(sales, df_item_cats, how='left', on='item_category_id')\n",
    "    # Create \"grid\" with columns\n",
    "    \n",
    "    # For every month we create a grid from all shops/items combinations from that month\n",
    "    grid = [] \n",
    "    for block_num in sales['date_block_num'].unique():\n",
    "        cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "        cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))\n",
    "    \n",
    "    # Turn the grid into a dataframe\n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "    \n",
    "    # Groupby data to get shop-item-month aggregates\n",
    "    gb = sales.groupby(index_cols, as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "    gb.rename(columns={'item_cnt_day': 'target'}, inplace=True)\n",
    "    \n",
    "    # Fix column names\n",
    "    # Join it to the grid\n",
    "    all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)#0.334?\n",
    "    \n",
    "    # Same as above but with shop-month aggregates\n",
    "    gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "    gb.rename(columns={'item_cnt_day': 'target_shop'}, inplace=True)\n",
    "    all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "    \n",
    "    # Same as above but with item-month aggregates\n",
    "    gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "    gb.rename(columns={'item_cnt_day': 'target_item'}, inplace=True)\n",
    "    all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)    \n",
    "    \n",
    "    ### Advanced features statistics for price\n",
    "    # mean price\n",
    "    if ENABLE_PRICE_STATS:\n",
    "        gb = sales.groupby(['item_id', 'shop_id', 'date_block_num'],as_index=False).agg({'item_price':'mean'})\n",
    "        gb.columns = ['item_id', 'shop_id', 'date_block_num', 'item_price_mean']\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'shop_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        all_data['sale_total'] = all_data['target'] * all_data['item_price_mean']\n",
    "        \n",
    "    if enable_subtype_code_mean:\n",
    "        # min/max price by 'subtype_code'\n",
    "        all_data = pd.merge(all_data, sales[['item_id','item_category_id']].drop_duplicates(), how='left', on='item_id')\n",
    "        all_data = pd.merge(all_data,  df_item_cats[['item_category_id', 'subtype_code']], how='left', on='item_category_id')\n",
    "        gb = sales.groupby(['subtype_code', 'shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "        gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "        gb.columns = ['subtype_code', 'shop_id', 'date_block_num', 'target_subtype_code']\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['subtype_code', 'shop_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        all_data.drop(columns=['item_category_id', 'subtype_code'], inplace=True)\n",
    "    \n",
    "    df_test_concat = df_test.drop(columns=['ID'])\n",
    "    df_test_concat['date_block_num'] = 34\n",
    "    all_data, TRAIN_SIZE = concat_df(all_data, df_test_concat)\n",
    "    \n",
    "    all_data = downcast_dtypes(all_data)\n",
    "#     del grid, gb, df_test_concat, sales, df_test\n",
    "#     gc.collect();\n",
    "    return all_data, TRAIN_SIZE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare historical lags item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "def get_lags(df):\n",
    "    \n",
    "    # List of columns that we will use to create lags\n",
    "    cols_to_rename = list(df.columns.difference(index_cols)) \n",
    "    \n",
    "    df = df.copy()\n",
    "    for month_shift in tqdm.tqdm(shift_range):\n",
    "        train_shift = df[index_cols + cols_to_rename].copy()\n",
    "        \n",
    "        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "        \n",
    "        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "        train_shift = train_shift.rename(columns=foo)\n",
    "    \n",
    "        df = pd.merge(df, train_shift, on=index_cols, how='left').fillna(0)\n",
    "    \n",
    "    del train_shift\n",
    "    \n",
    "    # try to delete medians\n",
    "    df.drop(columns=['target_shop', 'target_item'], inplace=True)\n",
    "    if ENABLE_PRICE_STATS:\n",
    "        df.drop(columns=['item_price_mean', 'sale_total'], inplace=True)\n",
    "        \n",
    "    if enable_subtype_code_mean:    \n",
    "        df.drop(columns=['target_subtype_code'], inplace=True)\n",
    "    \n",
    "    # List of all lagged features\n",
    "    fit_cols = [col for col in df.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "    # We will drop these at fitting stage\n",
    "    to_drop_cols = list(set(list(df.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "    \n",
    "    # Category for each item\n",
    "    item_category_mapping = df_items[['item_id','item_category_id']].drop_duplicates()\n",
    "    \n",
    "    df = pd.merge(df, item_category_mapping, how='left', on='item_id')\n",
    "    df = pd.merge(df, df_item_cats, how='left', on='item_category_id')\n",
    "    df = pd.merge(df, df_shops, how='left', on='shop_id')\n",
    "    df = pd.merge(df, df_items_txt, how='left', on='item_id')\n",
    "    \n",
    "#     months = df_train.groupby(['date_block_num'], as_index=False)['month'].mean()\n",
    "    \n",
    "#     df = pd.merge(df, months, how='left', on='date_block_num')\n",
    "    \n",
    "#     df['month'] = ((df['date_block_num'] % 12)+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generating categorical interaction feature for experiment/education\n",
    "    # all_data['item_category_id_city_code'] = all_data['item_category_id'].astype(str) + '_' + all_data['city_code'].astype(str)\n",
    "    # all_data['item_category_id_city_code'] = LabelEncoder().fit_transform(all_data['item_category_id_city_code'])\n",
    "    return downcast_dtypes(df)\n",
    "    # del df_items_txt\n",
    "    # gc.collect();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train/validation validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def trim_12(df):\n",
    "    return df[df['date_block_num'] >= 12]\n",
    "\n",
    "# TODO should we have subtype_code as it encodes item_category_id\n",
    "cat_columns = ['item_id', \n",
    "               'shop_id', \n",
    "               'item_category_id', 'type_code', 'subtype_code','city_code']#, \n",
    "               #'item_category_id_city_code]\n",
    "\n",
    "\n",
    "def split_train_and_postprocess_for_linear(X_train, Y_train, X_test):\n",
    "    Y_train = Y_train.reset_index(drop=True)\n",
    "    \n",
    "    df_all_data, train_size = concat_df(X_train, X_test)\n",
    "    \n",
    "    for cat_column in cat_columns:\n",
    "        df_all_data[cat_column] = df_all_data[cat_column].astype('category')\n",
    "             \n",
    "    df_cat = df_all_data[cat_columns + ['date_block_num']]\n",
    "    df_scale = df_all_data[list(set(df_all_data.columns) - set(cat_columns))]\n",
    "      \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_scale)\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(df_cat[cat_columns])\n",
    "    \n",
    "    train_cat, X_test_cat = divide_df(df_cat, train_size)\n",
    "    X_test_cat = X_test_cat.drop(columns=['date_block_num'])\n",
    "    \n",
    "    max_date = train_cat['date_block_num'].max()\n",
    "               \n",
    "    cv_X_test_cat = train_cat[train_cat['date_block_num'] == max_date].drop(columns=['date_block_num'])\n",
    "    cv_X_train_cat = train_cat[train_cat['date_block_num'] < max_date].drop(columns=['date_block_num'])\n",
    "    \n",
    "    train_scale, X_test_scale = divide_df(df_scale, train_size)\n",
    "    cv_X_test_scale = train_scale[train_scale['date_block_num'] == max_date]\n",
    "    cv_Y_test = Y_train[train_scale['date_block_num'] == max_date]\n",
    "    \n",
    "    cv_Y_train = Y_train[train_scale['date_block_num'] < max_date]\n",
    "    cv_X_train_scale = train_scale[train_scale['date_block_num'] < max_date]\n",
    "               \n",
    "    # scaling\n",
    "    X_test_scale = scaler.transform(X_test_scale)\n",
    "    cv_X_test_scale = scaler.transform(cv_X_test_scale)\n",
    "    cv_X_train_scale = scaler.transform(cv_X_train_scale)\n",
    "    \n",
    "    X_test_cat = encoder.transform(X_test_cat)\n",
    "    X_test_scale = csr_matrix(X_test_scale)\n",
    "    X_test = hstack((X_test_cat, X_test_scale))\n",
    "               \n",
    "    cv_X_test_cat = encoder.transform(cv_X_test_cat)\n",
    "    cv_X_test_scale = csr_matrix(cv_X_test_scale)\n",
    "    cv_X_test = hstack((cv_X_test_cat, cv_X_test_scale))\n",
    "    \n",
    "    cv_X_train_cat = encoder.transform(cv_X_train_cat)    \n",
    "    cv_X_train_scale = csr_matrix(cv_X_train_scale)\n",
    "    cv_X_train = hstack((cv_X_train_cat, cv_X_train_scale))\n",
    "               \n",
    "    return cv_X_train, cv_Y_train, cv_X_test, cv_Y_test,X_test\n",
    "\n",
    "def preprocess_for_linear(X_train, X_test):\n",
    "    df_all_data, train_size = concat_df(X_train, X_test)\n",
    "    for cat_column in cat_columns:\n",
    "        df_all_data[cat_column] = df_all_data[cat_column].astype('category')\n",
    "            \n",
    "    df_cat = df_all_data[cat_columns]\n",
    "    df_scale = df_all_data[list(set(df_all_data.columns) - set(cat_columns))]\n",
    "      \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_scale)\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(df_cat)\n",
    "    \n",
    "    train_cat, X_test_cat = divide_df(df_cat, train_size)\n",
    "    train_scale, X_test_scale = divide_df(df_scale, train_size)\n",
    "               \n",
    "    # scaling\n",
    "    X_test_scale = scaler.transform(X_test_scale)\n",
    "    X_test_cat = encoder.transform(X_test_cat)\n",
    "    X_test = hstack((X_test_cat, X_test_scale))\n",
    "    \n",
    "    train_cat = encoder.transform(train_cat)\n",
    "    train_scale = scaler.transform(train_scale)\n",
    "    X_train = hstack((train_cat, train_scale))\n",
    "               \n",
    "    return X_train, X_test\n",
    "               \n",
    "def train_and_predict_lr(X, Y, X_test, model_name):\n",
    "    pkl_filename = f'{model_name}.pkl'\n",
    "    if pathlib.Path(pkl_filename).exists():\n",
    "        print(f'Reading model {pkl_filename} from file')\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            lr = pickle.load(file)\n",
    "    else:\n",
    "        lr = SGDRegressor()\n",
    "        lr.fit(X, Y)\n",
    "        print(f'Storing model {pkl_filename} in file')\n",
    "        #with open(pkl_filename, 'wb') as file:\n",
    "            #pickle.dump(lr, file)\n",
    "    pred_lr = lr.predict(X_test)\n",
    "    return pred_lr\n",
    "\n",
    "def tune_lr_hp(X_tr, Y_tr, X_ts, Y_ts, model='SGD'):\n",
    "    \n",
    "    pkl_filename = f'models/{model}.pkl'\n",
    "    if pathlib.Path(pkl_filename).exists():\n",
    "        print(f'Reading model from file {pkl_filename}')\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "    \n",
    "    X = vstack((X_tr, X_ts))\n",
    "    Y = pd.concat([Y_tr, Y_ts])    \n",
    "    print(f'X: {X.shape}, Y: {Y.shape}')\n",
    "    \n",
    "    def cv_generator():        \n",
    "        train_ind = list(range(0, X_tr.shape[0]))\n",
    "        test_ind = list(range(X_tr.shape[0], X_tr.shape[0] + X_ts.shape[0]))\n",
    "        \n",
    "        print(f'X_tr shape: {X_tr.shape}, X_ts shape: {X_ts.shape}')\n",
    "        print(f'train_ind=[{train_ind[0]}, {train_ind[-1]}], test_ind=[{test_ind[0]}, {test_ind[-1]}]')\n",
    "        yield train_ind, test_ind\n",
    "    \n",
    "    n_HP_points_to_test = 10\n",
    "    \n",
    "    params = {'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "              'l1_ratio': sp_uniform(loc=0.05, scale=0.9),\n",
    "              'alpha' : 10.0**-np.arange(1,7),\n",
    "              'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive']}\n",
    "    fit_params={}\n",
    "    \n",
    "    gs = RandomizedSearchCV(\n",
    "            estimator=SGDRegressor(), param_distributions=params, \n",
    "            n_iter=n_HP_points_to_test,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            cv=cv_generator(),\n",
    "            refit=True,\n",
    "            random_state=314,\n",
    "            verbose=False, n_jobs=2)\n",
    "    gs.fit(X, Y, **fit_params)\n",
    "    pd.DataFrame.from_dict(gs.cv_results_).to_csv('lr_cv.csv')\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(gs.best_estimator_, file)\n",
    "    return gs.best_estimator_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train lightgbm model with HP optimization and prediction\n",
    "to speedup review we are storing optimized hyperparams, model training should not take much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(all_data):\n",
    "    X = X.copy()\n",
    "    X['target'] = Y\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    print('X.index')\n",
    "    print(X.index)\n",
    "    for train_index, val_index in skf.split(X, X[regularize_column]):\n",
    "        df_train, df_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        group = df_train.groupby(regularize_column).target.mean()\n",
    "        X.loc[val_index, target_enc] = X.loc[val_index, regularize_column].map(group)\n",
    "    X[target_enc].fillna(0.3343, inplace=True)\n",
    "    return X[target_enc]\n",
    "\n",
    "def get_cv_time_series_split(all_data, n_splits=4):\n",
    "    min_date = all_data['date_block_num'].min()\n",
    "    max_date = all_data['date_block_num'].max()\n",
    "    print(f'min_date: {min_date}, max_date: {max_date}')\n",
    "    months = random.sample(range(min_date+9, max_date), n_splits)\n",
    "    for month in months:        \n",
    "        train_index = all_data[(all_data['date_block_num'] < month)].index\n",
    "        test_index = all_data[all_data['date_block_num'] == month].index\n",
    "        print(f'time series cv split month: {month}, train size: {len(train_index)}, test size: {len(test_index)}')\n",
    "        yield train_index.values, test_index.values\n",
    "\n",
    "def optimize_lgb_parameters(X, Y, X_val, Y_val, month=''):\n",
    "    params_file = f'models/lgb_parameters_{month}.json'\n",
    "    if pathlib.Path(params_file).exists():\n",
    "        with open(params_file) as f:\n",
    "            params = json.load(f)\n",
    "            print('Lgbm parameters read from file.')\n",
    "            print(params)\n",
    "        return params\n",
    "    else:\n",
    "        print('Lgbm HP optimization...')\n",
    "#         lgb_params ={'num_leaves': sp_randint(6, 50), \n",
    "#              'min_child_samples': sp_randint(100, 500), \n",
    "#              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "#              'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "#              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "#              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "#              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
    "#              'bagging_fraction': sp_uniform(loc=0.5, scale=0.45),\n",
    "#              'feature_fraction': sp_uniform(loc=0.5, scale=0.45),\n",
    "#              'min_data_in_leaf': sp_randint(0, 300),\n",
    "# #                 'learning_rate': [0.03]\n",
    "#                     }\n",
    "        lgb_params ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "        fit_params={\"early_stopping_rounds\":30, \n",
    "                    \"eval_metric\" : 'rmse', \n",
    "                    \"eval_set\" : [(X_val, Y_val)],\n",
    "                    'eval_names': ['valid'],\n",
    "                    'verbose': 100,\n",
    "#                     'categorical_feature': 'auto'\n",
    "                   }\n",
    "        \n",
    "        n_HP_points_to_test = 50\n",
    "        \n",
    "        clf = lgb.LGBMRegressor(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=-1, \n",
    "                                n_estimators=50)\n",
    "        X = X.reset_index(drop=True)\n",
    "        Y = Y.reset_index(drop=True)\n",
    "        cv_generator = get_cv_time_series_split(X)\n",
    "        gs = RandomizedSearchCV(\n",
    "            estimator=clf, param_distributions=lgb_params, \n",
    "            n_iter=n_HP_points_to_test,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            cv=cv_generator,\n",
    "            refit=True,\n",
    "            random_state=314,\n",
    "            verbose=False)\n",
    "        gs.fit(X, Y, **fit_params)\n",
    "        print(f'Best score reached: {gs.best_score_} with params: {gs.best_params_}')\n",
    "        print(f'Feature importance HP optimization: {gs.best_estimator_.feature_importances_}')\n",
    "#         print(f'Model: {gs.best_estimator_.model_to_string()}')\n",
    "        params = gs.best_params_\n",
    "        with open(params_file, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "        pd.DataFrame.from_dict(gs.cv_results_).to_csv(f'results/lgb_csv_{month}.csv')\n",
    "        return params\n",
    "\n",
    "def train_and_predict_lgbm(X, Y, X_test, params, month=''):\n",
    "    model_file = f'models/lgb_{month}.txt'\n",
    "    if pathlib.Path(model_file).exists():\n",
    "        print(f'reading lgb model from {model_file}')\n",
    "        model = lgb.Booster(model_file=model_file)\n",
    "    else:\n",
    "        print(f'training for {model_file} by parameters')\n",
    "        model = lgb.train(params, lgb.Dataset(X, label=Y), 100)\n",
    "        model.save_model(model_file)\n",
    "        \n",
    "    pred_lgb = model.predict(X_test)\n",
    "    return pred_lgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1dd1af033c4217bfa66663c66c0b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_data, TRAIN_SIZE = get_all_data(df_train)\n",
    "all_data = get_lags(all_data)\n",
    "\n",
    "# ???\n",
    "# for cat_column in cat_columns:\n",
    "#     all_data[cat_column] = all_data[cat_column].astype('category')\n",
    "\n",
    "train, X_test = divide_df(all_data, TRAIN_SIZE)\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "train = trim_12(train) \n",
    "\n",
    "item_ids_subset = get_items_subset(train)\n",
    "train = train[item_ids_subset]\n",
    "\n",
    "X_train = train.drop(columns=['target'])\n",
    "Y_train = train['target']\n",
    "\n",
    "max_date = X_train['date_block_num'].max()\n",
    "\n",
    "# IMPORTANT this is holdout validation test set\n",
    "cv_X_test = X_train[X_train['date_block_num'] == max_date]\n",
    "cv_Y_test = Y_train[X_train['date_block_num'] == max_date]\n",
    "\n",
    "cv_Y_train = Y_train[X_train['date_block_num'] < max_date]\n",
    "cv_X_train = X_train[X_train['date_block_num'] < max_date]\n",
    "\n",
    "del df_train, all_data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_result = {}\n",
    "def get_meta_featues_train(X_tr, Y_tr, X_ts):\n",
    "\n",
    "    dates_train = X_tr['date_block_num']\n",
    "    \n",
    "    meta_dates = [27, 28, 29, 30, 31, 32, 33]\n",
    "\n",
    "#     meta_dates = [27]\n",
    "    \n",
    "    dates_train_level2 = dates_train[dates_train.isin(meta_dates)]\n",
    "    \n",
    "    # That is how we get target for the 2nd level dataset\n",
    "    y_train_level2 = Y_tr[dates_train.isin(meta_dates)]\n",
    "    \n",
    "    # And here we create 2nd level feeature matrix, init it with zeros first\n",
    "    X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "    \n",
    "    # Fit N diverse models on those M chunks and predict for the chunk M+1. \n",
    "    # Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end.\n",
    "    for cur_block_num in meta_dates:\n",
    "        print(f'Ensembling month {cur_block_num}')\n",
    "        \n",
    "        '''\n",
    "            1. Split `X_train` into parts\n",
    "               Remember, that corresponding dates are stored in `dates_train` \n",
    "            2. Fit linear regression \n",
    "            3. Fit LightGBM and put predictions          \n",
    "            4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "               You can use `dates_train_level2` for it\n",
    "               Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "        '''      \n",
    "        \n",
    "        train_x = X_tr.loc[dates_train < cur_block_num]\n",
    "        train_y = Y_tr[dates_train < cur_block_num]\n",
    "        \n",
    "        test_x = X_tr.loc[dates_train == cur_block_num]\n",
    "        test_y = Y_tr[dates_train == cur_block_num]\n",
    "        \n",
    "        cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr, X_test_lr \\\n",
    "            = split_train_and_postprocess_for_linear(train_x, train_y, test_x)\n",
    "        \n",
    "        be = tune_lr_hp(cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr, model=f'lr_meta_{cur_block_num}')\n",
    "        pred_lr = be.predict(X_test_lr)\n",
    "        lr_rmse = mean_squared_error(test_y, pred_lr, squared=False)\n",
    "        meta_result[f'lr_rmse_{cur_block_num}'] = [lr_rmse]\n",
    "        \n",
    "        cv_max_date = train_x['date_block_num'].max()\n",
    "\n",
    "        # IMPORTANT this is holdout validation test set\n",
    "        cv_X_ts = train_x[train_x['date_block_num'] == cv_max_date]\n",
    "        cv_Y_ts = train_y[train_x['date_block_num'] == cv_max_date]\n",
    "        \n",
    "        cv_Y_tr = train_y[train_x['date_block_num'] < cv_max_date]\n",
    "        cv_X_tr = train_x[train_x['date_block_num'] < cv_max_date]        \n",
    "        \n",
    "        lgb_parameters = optimize_lgb_parameters(cv_X_tr, cv_Y_tr, cv_X_ts, cv_Y_ts, month=cur_block_num)\n",
    "        pred_lgb = train_and_predict_lgbm(train_x, train_y, test_x, lgb_parameters, month=cur_block_num)\n",
    "        lgb_rmse = mean_squared_error(test_y, pred_lgb, squared=False)\n",
    "        meta_result[f'lgb_rmse_{cur_block_num}'] = [lgb_rmse]\n",
    "                \n",
    "        X_train_level2[dates_train_level2==cur_block_num, 0] = pred_lr\n",
    "        X_train_level2[dates_train_level2==cur_block_num, 1] = pred_lgb\n",
    "        \n",
    "    \n",
    "    best_alpha = get_best_alpha(X_train_level2, y_train_level2)\n",
    "    meta_result['best_alpha'] = [best_alpha]\n",
    "    test_preds = get_mix(best_alpha, np.c_[pred_lr, pred_lgb])\n",
    "    rmse_mix = mean_squared_error(test_y, test_preds)\n",
    "    meta_result['rmse_mix'] = [rmse_mix]\n",
    "    \n",
    "    # Use all train data to fit models and get predictions for test.\n",
    "    be.fit(X_tr, Y_tr)\n",
    "    pred_lr = be.predict(X_ts)\n",
    "    pred_lgb = train_and_predict_lgbm(X_tr, Y_tr, X_ts, lgb_parameters, 33)\n",
    "    X_test_level2 = np.c_[pred_lr, pred_lgb] \n",
    "    \n",
    "    return X_train_level2, y_train_level2, X_test_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembling month 27\n",
      "Reading model from file models/lr_meta_27.pkl\n",
      "Lgbm parameters read from file.\n",
      "{'colsample_bytree': 0.46716991724263107, 'min_child_samples': 439, 'min_child_weight': 1, 'num_leaves': 46, 'reg_alpha': 1, 'reg_lambda': 50, 'subsample': 0.20651472131008397}\n",
      "reading lgb model from models/lgb_27.txt\n",
      "Ensembling month 28\n",
      "X: (5068102, 15736), Y: (5068102,)\n",
      "X_tr shape: (4810730, 15736), X_ts shape: (257372, 15736)\n",
      "train_ind=[0, 4810729], test_ind=[4810730, 5068101]\n",
      "Lgbm HP optimization...\n",
      "min_date: 12, max_date: 26\n",
      "time series cv split month: 24, train size: 3939517, test size: 306950\n",
      "time series cv split month: 25, train size: 4246467, test size: 284491\n",
      "time series cv split month: 21, train size: 2963799, test size: 329368\n",
      "time series cv split month: 22, train size: 3293167, test size: 316100\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.927842\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.908839\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[28]\tvalid's rmse: 0.913525\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[29]\tvalid's rmse: 0.904891\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.928532\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid's rmse: 0.905622\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.907628\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[42]\tvalid's rmse: 0.902986\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.936607\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[22]\tvalid's rmse: 0.922744\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.931261\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[45]\tvalid's rmse: 0.925483\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.926727\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.908033\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.90889\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[27]\tvalid's rmse: 0.901811\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.923626\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[22]\tvalid's rmse: 0.907773\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[33]\tvalid's rmse: 0.909018\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.904607\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.930107\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.920153\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[44]\tvalid's rmse: 0.918707\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.915881\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.924489\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.904259\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.907801\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.900304\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.916638\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.906254\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.908104\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.903325\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.928297\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.916744\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[40]\tvalid's rmse: 0.915529\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[47]\tvalid's rmse: 0.914805\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's rmse: 0.921066\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.912116\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.904661\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[27]\tvalid's rmse: 0.910226\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.918674\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.908369\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.908098\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.902329\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's rmse: 0.925131\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.911113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.91151\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.901548\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.919967\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.911966\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.9086\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.905974\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's rmse: 0.94208\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.928014\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.932748\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.927948\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.925273\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.920322\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[28]\tvalid's rmse: 0.920211\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[47]\tvalid's rmse: 0.916139\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.917125\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.901392\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.902033\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.898408\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.925657\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.919483\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.919311\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[42]\tvalid's rmse: 0.914768\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.926222\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.910443\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid's rmse: 0.91583\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[47]\tvalid's rmse: 0.909379\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.920564\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.908854\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.907596\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.902007\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.927284\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.904911\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[22]\tvalid's rmse: 0.919614\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[24]\tvalid's rmse: 0.904225\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.928422\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.919319\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[27]\tvalid's rmse: 0.921364\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[47]\tvalid's rmse: 0.918796\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.928897\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid's rmse: 0.919737\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.91426\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.914226\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's rmse: 0.924348\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.910899\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[33]\tvalid's rmse: 0.913918\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.905759\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.928014\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.920341\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.918315\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.918285\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.925564\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.906183\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[22]\tvalid's rmse: 0.912774\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[22]\tvalid's rmse: 0.901125\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.93899\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.92496\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[39]\tvalid's rmse: 0.931069\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid's rmse: 0.926216\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.927379\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.909139\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[24]\tvalid's rmse: 0.920188\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid's rmse: 0.904341\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.925528\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.905707\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.907574\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.898105\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.92774\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.920279\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.918517\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.918347\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.922223\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.907617\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.904796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.902167\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.926324\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.910694\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.910659\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[24]\tvalid's rmse: 0.902511\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.924551\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.907569\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.917135\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.904469\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.929905\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.909559\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[27]\tvalid's rmse: 0.913373\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.905012\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.934695\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.923438\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[31]\tvalid's rmse: 0.92973\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[35]\tvalid's rmse: 0.925025\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.922631\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.910761\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.915147\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.908075\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.923974\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[24]\tvalid's rmse: 0.90635\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid's rmse: 0.90856\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.903789\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's rmse: 0.921193\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.911032\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.909569\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[28]\tvalid's rmse: 0.907627\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.936232\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[22]\tvalid's rmse: 0.922731\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.930431\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.924029\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's rmse: 0.929482\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid's rmse: 0.906955\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.914123\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[40]\tvalid's rmse: 0.904189\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.914413\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.902797\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.90291\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.903383\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.925492\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.904355\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.912501\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[29]\tvalid's rmse: 0.903518\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.937927\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid's rmse: 0.92323\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.929714\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.923563\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.926829\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[22]\tvalid's rmse: 0.911042\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.913126\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid's rmse: 0.908917\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.926704\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.906988\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.916617\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[36]\tvalid's rmse: 0.902593\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.927185\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.915871\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[27]\tvalid's rmse: 0.914696\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[40]\tvalid's rmse: 0.913604\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's rmse: 0.926817\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.910307\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.910025\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.902681\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.924263\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.914703\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.909115\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid's rmse: 0.904955\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.925252\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.915924\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.918119\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.913866\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.920651\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.900895\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.906609\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[26]\tvalid's rmse: 0.897942\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.936424\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[21]\tvalid's rmse: 0.922509\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.930641\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tvalid's rmse: 0.924462\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    }
   ],
   "source": [
    "X_train_level2, y_train_level2, X_test_level2 = get_meta_featues_train(X_train, Y_train, X_test)\n",
    "del train\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_level2, y_train_level2)\n",
    "train_preds = lr.predict(X_train_level2)\n",
    "rmse_train_stacking = mean_squared_error(y_train_level2, train_preds)\n",
    "meta_result['rmse_train_stacking'] = [rmse_train_stacking]\n",
    "meta_result['cv_fraction'] = [cv_fraction]\n",
    "\n",
    "test_preds = lr.predict(X_test_level2)\n",
    "get_result_df(test_preds).to_csv('result_ens.csv', index=False)\n",
    "\n",
    "if pathlib.Path('results/meta_features_results.csv').exists():\n",
    "    metrics = pd.read_csv('results/meta_features_results.csv')\n",
    "else:\n",
    "    metrics = pd.DataFrame()\n",
    "metrics = pd.concat([metrics, pd.DataFrame(data={k: [round(v[0], 5)] for k,v in meta_result.items()})], ignore_index=True)\n",
    "metrics.to_csv('results/meta_features_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final train/validation/prediction setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# ROUND I \n",
    "# FINDING OPTIMAL PARAMETERS AND PARAMETER ALPHA FOR MIXING UP RESULTS OF LBG AND LINEAR MODELS\n",
    "\n",
    "lgb_parameters = optimize_lgb_parameters(cv_X_train, cv_Y_train, cv_X_test, cv_Y_test)\n",
    "\n",
    "# Use holdout validation test set to find optimal alpha\n",
    "\n",
    "pred_tree = train_and_predict_lgbm(cv_X_train, cv_Y_train, cv_X_test, lgb_parameters, 'lgb_model_validation')\n",
    "lgb_r2 = r2_score(cv_Y_test, pred_tree)\n",
    "lgb_rmse = mean_squared_error(cv_Y_test, pred_tree, squared=False)\n",
    "\n",
    "cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr, X_test_lr \\\n",
    "    = split_train_and_postprocess_for_linear(X_train, Y_train, X_test)\n",
    "\n",
    "# pred_lr = train_and_predict_lr(cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, 'lr_model_full')\n",
    "be = tune_lr_hp(cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr)\n",
    "pred_lr = be.predict(cv_X_test_lr)\n",
    "lr_r2 = r2_score(cv_Y_test_lr, pred_lr)\n",
    "lr_rmse = mean_squared_error(cv_Y_test_lr, pred_lr, squared=False)\n",
    "\n",
    "X_train_level2 = np.c_[pred_tree, pred_lr]\n",
    "\n",
    "best_alpha = get_best_alpha(X_train_level2, cv_Y_test)\n",
    "mix = get_mix(best_alpha, X_train_level2)\n",
    "mix_r2 = r2_score(cv_Y_test, mix)\n",
    "mix_rmse = mean_squared_error(cv_Y_test, mix, squared=False)\n",
    "\n",
    "results = {'cv_fraction':cv_fraction, \n",
    "           'lr_rmse': lr_rmse, 'lr_r2': lr_r2, \n",
    "           'lgb_rmse': lgb_rmse, 'lgb_r2': lgb_r2, \n",
    "           'mix_rmse': mix_rmse, 'mix_r2': mix_r2, \n",
    "           'alfa': best_alpha}\n",
    "print(results)\n",
    "del cv_X_train, cv_Y_train, cv_X_test, cv_Y_test, cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr,\\\n",
    "    pred_lr, pred_tree\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store and display validation metruics results\n",
    "results_metrics = 'results_metrics.csv'\n",
    "columns=['lr_rmse', 'lr_r2', 'lgb_rmse', 'lgb_r2', 'mix_rmse', 'mix_r2', 'alfa']\n",
    "\n",
    "if pathlib.Path(results_metrics).exists():\n",
    "    metrics = pd.read_csv(results_metrics)\n",
    "else:\n",
    "    metrics = pd.DataFrame(columns=['cv_fraction'] + columns)\n",
    "\n",
    "metrics = pd.concat([metrics, pd.DataFrame(results, index=[len(metrics)+1])])   \n",
    "metrics.to_csv(results_metrics, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n",
    "for cv_fraction in metrics['cv_fraction'].unique():\n",
    "    ax =plt.axes([0, 0, 2, 2])\n",
    "    for col in columns:\n",
    "        sub_metrix = metrics[metrics['cv_fraction'] == cv_fraction]\n",
    "        ax.plot(sub_metrix.index, sub_metrix[col], marker='.', linestyle='-', ms=12, label=col)\n",
    "        for i,j in zip(sub_metrix.index,metrics[col]):\n",
    "            ax.annotate(str(round(j,4)),xy=(i,j))\n",
    "        plt.xlabel(f'cv_fraction: {cv_fraction}')\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ROUND II\n",
    "# TRAIN MODELS ON COMPLETE DATA SET AND GETTING PREDICTIONS FOR SUBMITING\n",
    "X_train_lr, X_test_lr = preprocess_for_linear(X_train, X_test)\n",
    "# pred_lr = train_and_predict_lr(X_train_lr, Y_train, X_test_lr, 'lr_model_full')\n",
    "be.fit(X_train_lr, Y_train)\n",
    "pred_lr = be.predict(X_test_lr)\n",
    "\n",
    "del X_train_lr, X_test_lr\n",
    "gc.collect();\n",
    "\n",
    "pred_tree = train_and_predict_lgbm(X_train, Y_train, X_test, lgb_parameters, 'lgb_model_train')\n",
    "\n",
    "X_train_level2 = np.c_[pred_tree, pred_lr]\n",
    "\n",
    "result = get_mix(best_alpha, X_train_level2)\n",
    "\n",
    "get_result_df(result).to_csv('result_lgb+lr.csv', index=False)\n",
    "# Your public and private LB scores are: 0.962927 and 0.963253.\n",
    "get_result_df(pred_tree).to_csv('result_lgb.csv', index=False)\n",
    "print(\"DONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(time.time() - start)/60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
