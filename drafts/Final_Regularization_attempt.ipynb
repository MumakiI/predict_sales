{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Future Sales\n",
    "\n",
    "## How to\n",
    "\n",
    "You don't have to do anything special, pretrained models are provided. Simply clone repo, run this notebook and enjoy. \n",
    "\n",
    "Can send result_lgb+lr.csv file for evaluation\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Notebook was created in typical anaconda environment\n",
    "\n",
    "Uncomment and run cell below. to install lightgbm (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import product\n",
    "import gc\n",
    "import tqdm.notebook as tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack, vstack\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "def downcast_dtypes(df):    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def concat_df(train_data, test_data):\n",
    "    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True), train_data.shape[0]\n",
    "\n",
    "def divide_df(all_data, train_size):\n",
    "    if 'target' in all_data:\n",
    "        return all_data.loc[:train_size-1], all_data.loc[train_size:].drop(['target'], axis=1)\n",
    "    return all_data.loc[:train_size-1], all_data.loc[train_size:]\n",
    "\n",
    "def get_result_df(predict):\n",
    "    result = pd.DataFrame(data={'ID': range(0, 214200), 'item_cnt_month': predict})\n",
    "    result['item_cnt_month'] = result['item_cnt_month'].clip(0, 20)\n",
    "    return result\n",
    "\n",
    "def get_mix(alpha, X):\n",
    "    return (alpha * X[:,0]) + ((1-alpha) * X[:,1])\n",
    "\n",
    "def get_best_alpha(X_train_level2, target):\n",
    "    alphas_to_try = np.linspace(0, 1, 1001)\n",
    "    max_r2 = 0\n",
    "    best_alpha = 1\n",
    "    \n",
    "    for alpha in alphas_to_try:\n",
    "        mix = get_mix(alpha, X_train_level2)\n",
    "        r2 = r2_score(target, mix)\n",
    "        if max_r2 < r2:\n",
    "            max_r2 = r2\n",
    "            best_alpha = alpha\n",
    "    return best_alpha\n",
    "\n",
    "# this factor is needed for pipeline testing, make < 1 to reduce amount of data in work\n",
    "cv_fraction = 1\n",
    "def get_items_subset(X_train): \n",
    "    item_set = X_train['item_id'].unique()\n",
    "    #random.shuffle(item_set)\n",
    "    l = int(len(item_set) * cv_fraction)\n",
    "    cv_item_set = item_set[:l]\n",
    "    return X_train['item_id'].isin(cv_item_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('sales_train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_shops = pd.read_csv('shops.csv')\n",
    "df_items = pd.read_csv('items.csv')\n",
    "df_item_cats = pd.read_csv('item_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing item_cnt_day outliers\n",
    "def remove_outlier(df_train, col):\n",
    "    Q1 = df_train[col].quantile(0.25)\n",
    "    Q3 = df_train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = (df_train[col] > Q3+1.5*IQR)\n",
    "    df_train[outliers].shape\n",
    "    df_train.drop(df_train[outliers].index, inplace=True)\n",
    "#remove_outlier(df_train, 'item_cnt_day')\n",
    "#remove_outlier(df_train, 'item_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MONTH\n",
    "\n",
    "df_train['month'] = pd.to_datetime(df_train['date'], format='%d.%m.%Y').dt.month\n",
    "# TODO add week, then add mean by week?\n",
    "\n",
    "# FIX SHOPS\n",
    "\n",
    "# Якутск Орджоникидзе, 56\n",
    "df_train.loc[df_train.shop_id == 0, 'shop_id'] = 57\n",
    "df_test.loc[df_test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "df_train.loc[df_train.shop_id == 1, 'shop_id'] = 58\n",
    "df_test.loc[df_test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "df_train.loc[df_train.shop_id == 10, 'shop_id'] = 11\n",
    "df_test.loc[df_test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create igem categories and cities from shops\n",
    "\n",
    "# Extract type and sub type code\n",
    "df_item_cats['split'] = df_item_cats['item_category_name'].str.split('-')\n",
    "df_item_cats['type'] = df_item_cats['split'].map(lambda x: x[0].strip())\n",
    "df_item_cats['type_code'] = LabelEncoder().fit_transform(df_item_cats['type'])\n",
    "# if subtype is nan then type\n",
    "df_item_cats['subtype'] = df_item_cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "df_item_cats['subtype_code'] = LabelEncoder().fit_transform(df_item_cats['subtype'])\n",
    "\n",
    "# features = 10\n",
    "# tfidf = TfidfVectorizer(max_features=features)\n",
    "# df_item_cats['item_category_name'] = df_item_cats['item_category_name'].replace('[0-9!\"\\?\\.)(,\\+\\*\\[\\]/:\\-\\'&]', ' ', regex=True)\n",
    "# txtFeatures = pd.DataFrame(tfidf.fit_transform(df_item_cats['item_category_name']).toarray())\n",
    "# cols = txtFeatures.columns\n",
    "\n",
    "df_item_cats = df_item_cats[['item_category_id','type_code', 'subtype_code']]\n",
    "# for i in range(features):\n",
    "#     df_item_cats['item_category_tfidf_' + str(i)] = txtFeatures[cols[i]]\n",
    "\n",
    "# Extract city\n",
    "df_shops.loc[df_shops['shop_name'] == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "df_shops['city'] = df_shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "df_shops.loc[df_shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "\n",
    "# add distance to Moscow?\n",
    "\n",
    "df_shops['city_code'] = LabelEncoder().fit_transform(df_shops['city'])\n",
    "df_shops = df_shops[['shop_id','city_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aimar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Processing text features\n",
    "\n",
    "df_items_txt = df_items[['item_id', 'item_name']]\n",
    "df_items_txt['item_name'] = df_items_txt['item_name'].replace('[0-9!\"\\?\\.)(,\\+\\*\\[\\]/:\\-\\'&]', ' ', regex=True)\n",
    "\n",
    "features = 10\n",
    "tfidf = TfidfVectorizer(max_features=features)\n",
    "df_items_txt['item_name_len'] = df_items_txt['item_name'].map(len)  \n",
    "df_items_txt['item_name_wc'] = df_items_txt['item_name'].map(lambda x: len(str(x).split(' '))) \n",
    "txtFeatures = pd.DataFrame(tfidf.fit_transform(df_items_txt['item_name']).toarray())\n",
    "cols = txtFeatures.columns\n",
    "\n",
    "for i in range(features):\n",
    "    df_items_txt['item_name_tfidf_' + str(i)] = txtFeatures[cols[i]]\n",
    "\n",
    "df_items_txt.drop(columns=['item_name'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare mean encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = df_train.copy()\n",
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n",
    "gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "gb.rename(columns={'item_cnt_day': 'target'}, inplace=True)\n",
    "\n",
    "# Fix column names\n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "gb.rename(columns={'item_cnt_day': 'target_shop'}, inplace=True)\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "gb['item_cnt_day'] = gb['item_cnt_day'].clip(0, 20)\n",
    "gb.rename(columns={'item_cnt_day': 'target_item'}, inplace=True)\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "df_test_concat = df_test.drop(columns=['ID'])\n",
    "df_test_concat['date_block_num'] = 34\n",
    "all_data, TRAIN_SIZE = concat_df(all_data, df_test_concat)\n",
    "\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb, df_test_concat, sales, df_test\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare historical lags item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a71e93b57e4417bfeedd369503a9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm.tqdm(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# try to delete medians\n",
    "all_data.drop(columns=['target_shop', 'target_item'], inplace=True)\n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = df_items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = pd.merge(all_data, df_item_cats, how='left', on='item_category_id')\n",
    "all_data = pd.merge(all_data, df_shops, how='left', on='shop_id')\n",
    "all_data = pd.merge(all_data, df_items_txt, how='left', on='item_id')\n",
    "\n",
    "# generating categorical interaction feature for experiment/education\n",
    "# all_data['item_category_id_city_code'] = all_data['item_category_id'].astype(str) + '_' + all_data['city_code'].astype(str)\n",
    "# all_data['item_category_id_city_code'] = LabelEncoder().fit_transform(all_data['item_category_id_city_code'])\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del df_items_txt\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train/validation validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def trim_12(df):\n",
    "    return df[df['date_block_num'] >= 12]\n",
    "\n",
    "# TODO should we have subtype_code as it encodes item_category_id\n",
    "cat_columns = ['item_id', 'shop_id', 'item_category_id', 'type_code', 'subtype_code','city_code']#, \n",
    "               #'item_category_id_city_code]\n",
    "\n",
    "\n",
    "def split_train_and_postprocess_for_linear(X_train, Y_train, X_test):\n",
    "    Y_train = Y_train.reset_index(drop=True)\n",
    "    \n",
    "    df_all_data, train_size = concat_df(X_train, X_test)\n",
    "    \n",
    "    for cat_column in cat_columns:\n",
    "        df_all_data[cat_column] = df_all_data[cat_column].astype('category')\n",
    "             \n",
    "    df_cat = df_all_data[cat_columns + ['date_block_num']]\n",
    "    df_scale = df_all_data[list(set(df_all_data.columns) - set(cat_columns))]\n",
    "      \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_scale)\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(df_cat[cat_columns])\n",
    "    \n",
    "    train_cat, X_test_cat = divide_df(df_cat, train_size)\n",
    "    X_test_cat = X_test_cat.drop(columns=['date_block_num'])\n",
    "    \n",
    "    max_date = train_cat['date_block_num'].max()\n",
    "               \n",
    "    cv_X_test_cat = train_cat[train_cat['date_block_num'] == max_date].drop(columns=['date_block_num'])\n",
    "    cv_X_train_cat = train_cat[train_cat['date_block_num'] < max_date].drop(columns=['date_block_num'])\n",
    "    \n",
    "    train_scale, X_test_scale = divide_df(df_scale, train_size)\n",
    "    cv_X_test_scale = train_scale[train_scale['date_block_num'] == max_date]\n",
    "    cv_Y_test = Y_train[train_scale['date_block_num'] == max_date]\n",
    "    \n",
    "    cv_Y_train = Y_train[train_scale['date_block_num'] < max_date]\n",
    "    cv_X_train_scale = train_scale[train_scale['date_block_num'] < max_date]\n",
    "               \n",
    "    # scaling\n",
    "    X_test_scale = scaler.transform(X_test_scale)\n",
    "    cv_X_test_scale = scaler.transform(cv_X_test_scale)\n",
    "    cv_X_train_scale = scaler.transform(cv_X_train_scale)\n",
    "    \n",
    "    X_test_cat = encoder.transform(X_test_cat)\n",
    "    X_test_scale = csr_matrix(X_test_scale)\n",
    "    X_test = hstack((X_test_cat, X_test_scale))\n",
    "               \n",
    "    cv_X_test_cat = encoder.transform(cv_X_test_cat)\n",
    "    cv_X_test_scale = csr_matrix(cv_X_test_scale)\n",
    "    cv_X_test = hstack((cv_X_test_cat, cv_X_test_scale))\n",
    "    \n",
    "    cv_X_train_cat = encoder.transform(cv_X_train_cat)    \n",
    "    cv_X_train_scale = csr_matrix(cv_X_train_scale)\n",
    "    cv_X_train = hstack((cv_X_train_cat, cv_X_train_scale))\n",
    "               \n",
    "    return cv_X_train, cv_Y_train, cv_X_test, cv_Y_test,X_test\n",
    "\n",
    "def preprocess_for_linear(X_train, X_test):\n",
    "    df_all_data, train_size = concat_df(X_train, X_test)\n",
    "    for cat_column in cat_columns:\n",
    "        df_all_data[cat_column] = df_all_data[cat_column].astype('category')\n",
    "            \n",
    "    df_cat = df_all_data[cat_columns]\n",
    "    df_scale = df_all_data[list(set(df_all_data.columns) - set(cat_columns))]\n",
    "      \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_scale)\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(df_cat)\n",
    "    \n",
    "    train_cat, X_test_cat = divide_df(df_cat, train_size)\n",
    "    train_scale, X_test_scale = divide_df(df_scale, train_size)\n",
    "               \n",
    "    # scaling\n",
    "    X_test_scale = scaler.transform(X_test_scale)\n",
    "    X_test_cat = encoder.transform(X_test_cat)\n",
    "    X_test = hstack((X_test_cat, X_test_scale))\n",
    "    \n",
    "    train_cat = encoder.transform(train_cat)\n",
    "    train_scale = scaler.transform(train_scale)\n",
    "    X_train = hstack((train_cat, train_scale))\n",
    "               \n",
    "    return X_train, X_test\n",
    "               \n",
    "def train_and_predict_lr(X, Y, X_test, model_name):\n",
    "    pkl_filename = f'{model_name}.pkl'\n",
    "    if pathlib.Path(pkl_filename).exists():\n",
    "        print(f'Reading model {pkl_filename} from file')\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            lr = pickle.load(file)\n",
    "    else:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X, Y)\n",
    "        print(f'Storing model {pkl_filename} in file')\n",
    "        #with open(pkl_filename, 'wb') as file:\n",
    "            #pickle.dump(lr, file)\n",
    "    pred_lr = lr.predict(X_test)\n",
    "    return pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train lightgbm model with HP optimization and prediction\n",
    "to speedup review we are storing optimized hyperparams, model training should not take much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularize on X\n",
    "fold_cal = 'fold'\n",
    "regularize_column = 'item_id'\n",
    "target_enc = regularize_column + '_target_enc'\n",
    "\n",
    "reg_key_columns = ['date_block_num', 'item_id']\n",
    "reg_key_enc = '_'.join(reg_key_columns) + '_target_enc'\n",
    "\n",
    "def regularize(X, Y):\n",
    "    X = X.copy()\n",
    "    X['target'] = Y\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    print('X.index')\n",
    "    print(X.index)\n",
    "    for train_index, val_index in skf.split(X, X[regularize_column]):\n",
    "        df_train, df_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        group = df_train.groupby(regularize_column).target.mean()\n",
    "        X.loc[val_index, target_enc] = X.loc[val_index, regularize_column].map(group)\n",
    "    X[target_enc].fillna(0.3343, inplace=True)\n",
    "    return X[target_enc]\n",
    "\n",
    "def mean_encode_and_regularize(X_tr, Y_tr, X_val, Y_val):\n",
    "    X_val = X_val.copy() \n",
    "    X_val['target'] = Y_val\n",
    "    \n",
    "    # estimate encodings on X_train\n",
    "    # map them to (X_train?) and X_val\n",
    "    X_tr = X_tr.copy()\n",
    "    X_tr['target'] = Y_tr\n",
    "    gb = X_tr.groupby(regularize_column)['target'].mean()\n",
    "    X_val[target_enc] = X_val[regularize_column].map(gb)\n",
    "    X_val[target_enc].fillna(0.3343, inplace=True)\n",
    "    \n",
    "    return regularize(X_tr, Y_tr), X_val[target_enc] \n",
    "\n",
    "\n",
    "def combine_encoded_K_fold(df, Y, n_splits=4):\n",
    "    df = df.copy()\n",
    "    df_combined = pd.DataFrame()\n",
    "   \n",
    "    min_date = df['date_block_num'].min()\n",
    "    max_date = df['date_block_num'].max()\n",
    "    print(f'min_date: {min_date}, max_date: {max_date}')\n",
    "    months = random.sample(range(min_date+12, max_date), n_splits)\n",
    "    print('Months for folds' + str(months))\n",
    "    for fold, month in enumerate(months):\n",
    "        print(f'Mean encode fold:{fold}, month: {month}')\n",
    "        df.loc[df['date_block_num'] <= month, 'fold'] = fold\n",
    "        \n",
    "        train_ind = df['date_block_num'] < month\n",
    "        val_ind = df['date_block_num'] == month\n",
    "        df_train, df_val = df[train_ind], df[val_ind]\n",
    "        train_enc_reg, test_enc = mean_encode_and_regularize(df_train, Y[train_ind],\n",
    "                                                             df_val, Y[val_ind])\n",
    "        df_train = pd.concat([df_train, train_enc_reg, Y[train_ind]], axis=1)\n",
    "        df_val = pd.concat([df_val, test_enc, Y[val_ind]], axis=1)\n",
    "        df_combined = pd.concat([df_combined, df_train, df_val], sort=True)\n",
    "        \n",
    "    print('Mean encoded columns:')\n",
    "    print(df_combined.columns)\n",
    "    df_combined.reset_index(drop=True, inplace=True)\n",
    "    Y = df_combined['target']\n",
    "    df_combined.drop(columns=['target'], inplace=True)\n",
    "    return df_combined, Y\n",
    "        \n",
    "    \n",
    "def get_cv_time_series_split(all_data):\n",
    "    folds = all_data['fold'].unique()\n",
    "    print(all_data.index)\n",
    "    splits = []\n",
    "    for fold in folds: \n",
    "        if np.isnan(fold):\n",
    "            print('skipping null fold')\n",
    "            continue\n",
    "        max_date = all_data[all_data['fold'] == fold]['date_block_num'].max()\n",
    "        print(f'get_cv_time_series_split: fold={fold}, max_date={max_date}')\n",
    "        \n",
    "        train_index = all_data[(all_data['fold'] == fold) & (all_data['date_block_num'] < max_date)].index\n",
    "        test_index = all_data[(all_data['fold'] == fold) & (all_data['date_block_num'] == max_date)].index\n",
    "        \n",
    "        print(f'time series cv split month: {max_date}, train size: {len(train_index)}, test size: {len(test_index)}')\n",
    "        splits.append((train_index.values, test_index.values))\n",
    "    all_data.drop(columns=['fold'], inplace=True)                                                       \n",
    "    for split in splits:\n",
    "        yield split\n",
    "                                                \n",
    "                                                \n",
    "\n",
    "def optimize_lgb_parameters(X, Y, X_val, Y_val):\n",
    "    \n",
    "    if pathlib.Path('lgb_parameters.json').exists():\n",
    "        with open('lgb_parameters.json') as f:\n",
    "            params = json.load(f)\n",
    "            print('Lgbm parameters read from file.')\n",
    "            print(params)\n",
    "        return params\n",
    "    else:\n",
    "        print('Lgbm HP optimization...')\n",
    "        \n",
    "        X_val = X_val.reset_index(drop=True)\n",
    "        Y_val = Y_val.reset_index(drop=True)\n",
    "        \n",
    "        X_copy = X.copy()\n",
    "        X_copy['target'] = Y\n",
    "        gb = X_copy.groupby(regularize_column)['target'].mean()\n",
    "        X_val[target_enc] = X[regularize_column].map(gb)\n",
    "        X_val[target_enc].fillna(0.3343, inplace=True)\n",
    "        \n",
    "#         X_val = pd.concat([X_val, regularize(X_val, Y_val)], axis=1)\n",
    "        \n",
    "        lgb_params ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "        fit_params={\"early_stopping_rounds\":30, \n",
    "                    \"eval_metric\" : 'rmse', \n",
    "                    \"eval_set\" : [(X_val, Y_val)],\n",
    "                    'eval_names': ['valid'],\n",
    "                    'verbose': 100,\n",
    "                    'categorical_feature': 'auto'}\n",
    "        \n",
    "        n_HP_points_to_test = 50\n",
    "        \n",
    "        clf = lgb.LGBMRegressor(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=-1, \n",
    "                                n_estimators=50)\n",
    "        X = X.reset_index(drop=True)\n",
    "        Y = Y.reset_index(drop=True)\n",
    "        \n",
    "        print(f'X shape before combine_encoded_K_fold ({X.shape[0]}, {X.shape[1]})')\n",
    "        \n",
    "        X_cv, Y = combine_encoded_K_fold(X, Y)\n",
    "        \n",
    "        print(f'X shape after combine_encoded_K_fold ({X_cv.shape[0]}, {X_cv.shape[1]})')\n",
    "        \n",
    "        cv_generator = get_cv_time_series_split(X_cv)\n",
    "        gs = RandomizedSearchCV(\n",
    "            estimator=clf, param_distributions=lgb_params, \n",
    "            n_iter=n_HP_points_to_test,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            cv=cv_generator,\n",
    "            refit=True,\n",
    "            random_state=314,\n",
    "            verbose=False)\n",
    "       \n",
    "        gs.fit(X_cv, Y, **fit_params)\n",
    "        print(f'Best score reached: {gs.best_score_} with params: {gs.best_params_}')\n",
    "        print(f'Feature importance HP optimization: {gs.best_estimator_.feature_importances_}')\n",
    "        params = gs.best_params_\n",
    "        #with open('lgb_parameters.json', 'w') as f:\n",
    "            #json.dump(params, f)\n",
    "        pd.DataFrame.from_dict(gs.cv_results_).to_csv('lgb_csv.csv')\n",
    "        return params\n",
    "\n",
    "def train_and_predict_lgbm(X, Y, X_test, params, model_name):\n",
    "    model_file = f'{model_name}.txt'\n",
    "    X_test_copy = X.copy()\n",
    "    X_test_copy['target'] = Y\n",
    "    gb = X_test_copy.groupby(regularize_column)['target'].mean()\n",
    "    X_test[target_enc] = X_test[regularize_column].map(gb)\n",
    "    X_test[target_enc].fillna(0.3343, inplace=True)\n",
    "    \n",
    "    X = X.reset_index(drop=True)\n",
    "    Y = Y.reset_index(drop=True)\n",
    "    \n",
    "    X = pd.concat([X, regularize(X, Y)], axis=1)\n",
    "    \n",
    "    print(f'train_and_predict_lgbm(): X.columns {X.columns}')\n",
    "    print(f'train_and_predict_lgbm(): X_test.columns {X_test.columns}')\n",
    "    \n",
    "    if pathlib.Path(model_file).exists():\n",
    "        print(f'reading lgb model {model_name} from file')\n",
    "        model = lgb.Booster(model_file=model_file)\n",
    "    else:\n",
    "        print(f'training {model_name} by parameters')\n",
    "        model = lgb.train(params, lgb.Dataset(X, label=Y), 100)\n",
    "        #model.save_model(model_file)\n",
    "    print(f'Feature importance prediction: {model.feature_importance()}')\n",
    "    pred_lgb = model.predict(X_test)\n",
    "    return pred_lgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, X_test = divide_df(all_data, TRAIN_SIZE)\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "train = trim_12(train) \n",
    "\n",
    "item_ids_subset = get_items_subset(train)\n",
    "train = train[item_ids_subset]\n",
    "\n",
    "X_train = train.drop(columns=['target'])\n",
    "Y_train = train['target']\n",
    "\n",
    "max_date = X_train['date_block_num'].max()\n",
    "\n",
    "# IMPORTANT this is holdout validation test set\n",
    "cv_X_test = X_train[X_train['date_block_num'] == max_date]\n",
    "cv_Y_test = Y_train[X_train['date_block_num'] == max_date]\n",
    "\n",
    "cv_Y_train = Y_train[X_train['date_block_num'] < max_date]\n",
    "cv_X_train = X_train[X_train['date_block_num'] < max_date]\n",
    "\n",
    "del train, df_train, all_data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final train/validation/prediction setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lgbm HP optimization...\n",
      "X shape before combine_encoded_K_fold (6186922, 37)\n",
      "min_date: 12, max_date: 32\n",
      "Months for folds[30, 31, 27, 24]\n",
      "Mean encode fold:0, month: 30\n",
      "X.index\n",
      "Int64Index([      0,       1,       2,       3,       4,       5,       6,\n",
      "                  7,       8,       9,\n",
      "            ...\n",
      "            5524832, 5524833, 5524834, 5524835, 5524836, 5524837, 5524838,\n",
      "            5524839, 5524840, 5524841],\n",
      "           dtype='int64', length=5524842)\n",
      "Mean encode fold:1, month: 31\n",
      "X.index\n",
      "Int64Index([      0,       1,       2,       3,       4,       5,       6,\n",
      "                  7,       8,       9,\n",
      "            ...\n",
      "            5753721, 5753722, 5753723, 5753724, 5753725, 5753726, 5753727,\n",
      "            5753728, 5753729, 5753730],\n",
      "           dtype='int64', length=5753731)\n",
      "Mean encode fold:2, month: 27\n",
      "X.index\n",
      "Int64Index([      0,       1,       2,       3,       4,       5,       6,\n",
      "                  7,       8,       9,\n",
      "            ...\n",
      "            4810720, 4810721, 4810722, 4810723, 4810724, 4810725, 4810726,\n",
      "            4810727, 4810728, 4810729],\n",
      "           dtype='int64', length=4810730)\n",
      "Mean encode fold:3, month: 24\n",
      "X.index\n",
      "Int64Index([      0,       1,       2,       3,       4,       5,       6,\n",
      "                  7,       8,       9,\n",
      "            ...\n",
      "            3939507, 3939508, 3939509, 3939510, 3939511, 3939512, 3939513,\n",
      "            3939514, 3939515, 3939516],\n",
      "           dtype='int64', length=3939517)\n",
      "Mean encoded columns:\n",
      "Index(['city_code', 'date_block_num', 'fold', 'item_category_id', 'item_id',\n",
      "       'item_id_target_enc', 'item_name_len', 'item_name_tfidf_0',\n",
      "       'item_name_tfidf_1', 'item_name_tfidf_2', 'item_name_tfidf_3',\n",
      "       'item_name_tfidf_4', 'item_name_tfidf_5', 'item_name_tfidf_6',\n",
      "       'item_name_tfidf_7', 'item_name_tfidf_8', 'item_name_tfidf_9',\n",
      "       'item_name_wc', 'shop_id', 'subtype_code', 'target',\n",
      "       'target_item_lag_1', 'target_item_lag_12', 'target_item_lag_2',\n",
      "       'target_item_lag_3', 'target_item_lag_4', 'target_item_lag_5',\n",
      "       'target_lag_1', 'target_lag_12', 'target_lag_2', 'target_lag_3',\n",
      "       'target_lag_4', 'target_lag_5', 'target_shop_lag_1',\n",
      "       'target_shop_lag_12', 'target_shop_lag_2', 'target_shop_lag_3',\n",
      "       'target_shop_lag_4', 'target_shop_lag_5', 'type_code'],\n",
      "      dtype='object')\n",
      "X shape after combine_encoded_K_fold (21036567, 39)\n",
      "RangeIndex(start=0, stop=21036567, step=1)\n",
      "get_cv_time_series_split: fold=0.0, max_date=30\n",
      "time series cv split month: 30, train size: 5524842, test size: 228889\n",
      "get_cv_time_series_split: fold=1.0, max_date=31\n",
      "time series cv split month: 31, train size: 5753731, test size: 214536\n",
      "get_cv_time_series_split: fold=2.0, max_date=27\n",
      "time series cv split month: 27, train size: 4810730, test size: 257372\n",
      "get_cv_time_series_split: fold=3.0, max_date=24\n",
      "time series cv split month: 24, train size: 3939517, test size: 306950\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.55494\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.55789\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56897\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56691\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.53403\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.53513\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.54569\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.55477\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19013\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19422\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.18909\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14174\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.57266\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58116\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58397\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.62375\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56585\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.57293\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58105\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.62312\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.20294\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19525\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.2155\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.15681\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.57656\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58102\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.5877\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.5888\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13898\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13679\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13937\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13965\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19624\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19145\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.29637\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.15081\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13689\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13616\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13694\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13731\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13663\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13593\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13683\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.18279\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.24225\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.1801\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.15756\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13692\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.136\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13713\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13723\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56115\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56243\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56764\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.52321\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13695\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13635\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13717\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13659\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13665\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13593\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13683\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.1369\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13695\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13865\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13717\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13659\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56256\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56539\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.57731\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58911\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13665\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13594\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13683\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13689\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.57654\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56283\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.63636\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58885\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13984\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.1397\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14015\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14083\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.20285\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19514\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.21531\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.15673\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56578\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.57284\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58105\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.62312\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13641\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13855\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13655\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13659\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.16988\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.23977\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19599\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.15697\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19099\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.1937\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.18984\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14179\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.17456\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.24292\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.20534\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14394\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.18359\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.21604\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.16999\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.15575\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13641\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13855\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13655\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13659\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13665\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13595\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13681\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13687\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56743\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58023\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58245\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.62203\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.17\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.24101\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.2113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.15705\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58062\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58301\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.5904\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.63\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56658\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56777\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.5736\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.52844\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13689\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13617\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13694\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13731\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56278\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56535\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.57677\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58962\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13695\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13597\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13704\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13731\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14021\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14172\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14237\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.54859\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.54944\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56265\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.57859\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13665\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13593\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13683\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.1369\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.61052\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58077\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58739\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58444\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13995\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13966\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14203\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.14243\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.20624\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.202\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.26846\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.1519\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.61102\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58102\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58769\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58885\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19633\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.19151\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.29697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.15085\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.56719\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.57997\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.58258\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.6227\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13699\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13608\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13721\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13731\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13694\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 1.13548\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    }
   ],
   "source": [
    "# ROUND I \n",
    "# FINDING OPTIMAL PARAMETERS AND PARAMETER ALPHA FOR MIXING UP RESULTS OF LBG AND LINEAR MODELS\n",
    "\n",
    "lgb_parameters = optimize_lgb_parameters(cv_X_train, cv_Y_train, cv_X_test, cv_Y_test)\n",
    "\n",
    "# Use holdout validation test set to find optimal alpha\n",
    "\n",
    "pred_tree = train_and_predict_lgbm(cv_X_train, cv_Y_train, cv_X_test, lgb_parameters, 'lgb_model_validation')\n",
    "lgb_r2 = r2_score(cv_Y_test, pred_tree)\n",
    "lgb_rmse = mean_squared_error(cv_Y_test, pred_tree, squared=False)\n",
    "\n",
    "cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr, X_test_lr \\\n",
    "    = split_train_and_postprocess_for_linear(X_train, Y_train, X_test)\n",
    "\n",
    "pred_lr = train_and_predict_lr(cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, 'lr_model_full')\n",
    "lr_r2 = r2_score(cv_Y_test_lr, pred_lr)\n",
    "lr_rmse = mean_squared_error(cv_Y_test_lr, pred_lr, squared=False)\n",
    "\n",
    "X_train_level2 = np.c_[pred_tree, pred_lr]\n",
    "\n",
    "best_alpha = get_best_alpha(X_train_level2, cv_Y_test)\n",
    "mix = get_mix(best_alpha, X_train_level2)\n",
    "mix_r2 = r2_score(cv_Y_test, mix)\n",
    "mix_rmse = mean_squared_error(cv_Y_test, mix, squared=False)\n",
    "\n",
    "results = {'cv_fraction':cv_fraction, \n",
    "           'lr_rmse': lr_rmse, 'lr_r2': lr_r2, \n",
    "           'lgb_rmse': lgb_rmse, 'lgb_r2': lgb_r2, \n",
    "           'mix_rmse': mix_rmse, 'mix_r2': mix_r2, \n",
    "           'alfa': best_alpha}\n",
    "print(results)\n",
    "del cv_X_train, cv_Y_train, cv_X_test, cv_Y_test, cv_X_train_lr, cv_Y_train_lr, cv_X_test_lr, cv_Y_test_lr\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store and display validation metruics results\n",
    "results_metrics = 'results_metrics.csv'\n",
    "columns=['lr_rmse', 'lr_r2', 'lgb_rmse', 'lgb_r2', 'mix_rmse', 'mix_r2', 'alfa']\n",
    "\n",
    "if pathlib.Path(results_metrics).exists():\n",
    "    metrics = pd.read_csv(results_metrics)\n",
    "else:\n",
    "    metrics = pd.DataFrame(columns=['cv_fraction'] + columns)\n",
    "\n",
    "metrics = pd.concat([metrics, pd.DataFrame(results, index=[len(metrics)+1])])   \n",
    "metrics.to_csv(results_metrics, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for cv_fraction in metrics['cv_fraction'].unique():\n",
    "    ax =plt.axes([0, 0, 2, 2])\n",
    "    for col in columns:\n",
    "        sub_metrix = metrics[metrics['cv_fraction'] == cv_fraction]\n",
    "        ax.plot(sub_metrix.index, sub_metrix[col], marker='.', linestyle='-', ms=12, label=col)\n",
    "        for i,j in zip(sub_metrix.index,metrics[col]):\n",
    "            ax.annotate(str(round(j,4)),xy=(i,j))\n",
    "        plt.xlabel(f'cv_fraction: {cv_fraction}')\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROUND II\n",
    "# TRAIN MODELS ON COMPLETE DATA SET AND GETTING PREDICTIONS FOR SUBMITING\n",
    "X_train_lr, X_test_lr = preprocess_for_linear(X_train, X_test)\n",
    "pred_lr = train_and_predict_lr(X_train_lr, Y_train, X_test_lr, 'lr_model_full')\n",
    "\n",
    "del X_train_lr, X_test_lr\n",
    "gc.collect();\n",
    "\n",
    "pred_tree = train_and_predict_lgbm(X_train, Y_train, X_test, lgb_parameters, 'lgb_model_train')\n",
    "\n",
    "\n",
    "X_train_level2 = np.c_[pred_tree, pred_lr]\n",
    "\n",
    "result = get_mix(best_alpha, X_train_level2)\n",
    "\n",
    "get_result_df(result).to_csv('result_lgb+lr.csv', index=False)\n",
    "# Your public and private LB scores are: 0.962927 and 0.963253.\n",
    "get_result_df(pred_tree).to_csv('result_lgb.csv', index=False)\n",
    "print(\"DONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "you can send result_lgb+lr.csv file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wher is month???\n",
    "X_train.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
