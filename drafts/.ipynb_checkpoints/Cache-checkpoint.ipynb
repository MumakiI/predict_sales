{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare historical lags item category\n",
    "if False:    \n",
    "    # Create \"grid\" with columns\n",
    "    index_cols = ['shop_id', 'item_category_id', 'date_block_num']\n",
    "    \n",
    "    sales = pd.merge(df_all, df_items[['item_id', 'item_category_id']], on='item_id', how='left')\n",
    "    \n",
    "    # For every month we create a grid from all shops/items combinations from that month\n",
    "    grid = [] \n",
    "    for block_num in sales['date_block_num'].unique():\n",
    "        cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "        cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_category_id'].unique()\n",
    "        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "    \n",
    "    # Turn the grid into a dataframe\n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "    \n",
    "    # Groupby data to get shop-item_category-month aggregates\n",
    "    gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb.rename(columns={'item_cnt_day': 'shop_item_cat_date_sum'}, inplace=True)\n",
    "    # Join it to the grid\n",
    "    df_all_grouped = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "    \n",
    "    # Groupby data to get shop-month aggregates\n",
    "    gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb.rename(columns={'item_cnt_day': 'shop_date_sum'}, inplace=True)\n",
    "    df_all_grouped = pd.merge(df_all_grouped, gb[['shop_id', 'date_block_num', 'shop_date_sum']], \n",
    "                              how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "    \n",
    "    # Groupby data to get item_category-month aggregates\n",
    "    gb = sales.groupby(['item_category_id', 'date_block_num'], as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb.rename(columns={'item_cnt_day': 'item_cat_date_sum'}, inplace=True)\n",
    "    df_all_grouped = pd.merge(df_all_grouped, gb[['item_category_id', 'date_block_num', 'item_cat_date_sum']], \n",
    "                              how='left', on=['item_category_id', 'date_block_num']).fillna(0)\n",
    "    \n",
    "    # TODO create item_cnt_day_by_item_cat weighted by items count in group\n",
    "    \n",
    "    \n",
    "    # List of columns that we will use to create lags\n",
    "    cols_to_rename = list(df_all_grouped.columns.difference(index_cols)) \n",
    "    \n",
    "    shift_range = [1, 2, 3, 4, 5, 12]\n",
    "    \n",
    "    for month_shift in tqdm.tqdm(shift_range):\n",
    "        train_shift = df_all_grouped[index_cols + cols_to_rename].copy()\n",
    "        \n",
    "        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "        \n",
    "        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "        train_shift = train_shift.rename(columns=foo)\n",
    "    \n",
    "        df_all_grouped = pd.merge(df_all_grouped, train_shift, on=index_cols, how='left').fillna(0)\n",
    "    \n",
    "    # we may have only historical data for target month, we don't know aggregations for it, hence deleting\n",
    "    df_all_grouped.drop(columns=['shop_item_cat_date_sum', 'item_cat_date_sum', 'shop_date_sum'], inplace=True)\n",
    "    \n",
    "    # merge history back to train/test set \n",
    "    df_all_gr_by_item_cat = pd.merge(sales, df_all_grouped, on=index_cols, how='left').fillna(0)\n",
    "    df_all_gr_by_item_cat.drop(columns=['date', 'item_price', 'item_id', 'item_cnt_day'], inplace=True)\n",
    "    df_all_gr_by_item_cat = downcast_dtypes(df_all_gr_by_item_cat)\n",
    "    \n",
    "    del grid, df_all_grouped, sales, train_shift\n",
    "    gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "lgb_params = {\n",
    "               'feature_fraction': [0.55, 0.65, 0.75, 0.85, 0.95],\n",
    "               'metric': ['rmse'],\n",
    "               'nthread':[-1], \n",
    "               'min_data_in_leaf': [2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10], \n",
    "               'bagging_fraction': [0.65, 0.75, 0.85], \n",
    "               'learning_rate': [0.1, 0.01, 0.03], \n",
    "               'objective': ['mse'], \n",
    "               'bagging_seed': [2**7], \n",
    "               'num_leaves': sp_randint(100, 1000),\n",
    "               'bagging_freq':[1],\n",
    "               'verbose':[0] \n",
    "              }\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'mse', \n",
    "            \"eval_set\" : [(cv_X_test, cv_Y_test)],\n",
    "            'eval_names': ['valid'],\n",
    "            #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "clf = lgb.LGBMRegressor(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=10)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=lgb_params, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='r2',\n",
    "    cv=4,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=False)\n",
    "gs.fit(cv_X_train, cv_Y_train, **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))\n",
    "\n",
    "lgb_params = {\n",
    "               'feature_fraction': 0.95,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 16, \n",
    "               'bagging_fraction': 0.85, \n",
    "               'learning_rate': 0.1, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 906,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=Y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "get_result_df(pred_lgb).to_csv('result_lgb_26_features.csv', index=False)\n",
    "# print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))\n",
    "# Your solution can be improved by a lot! Your public and private LB scores are: 1.319619 and 1.318699."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_and_postprocess_for_linear_bk(df_all_data):\n",
    "    cat_columns = ['item_id', 'shop_id', 'item_category_id', 'type_code', 'subtype_code','city_code']\n",
    "    for cat_column in cat_columns:\n",
    "        df_all_data[cat_column] = df_all_data[cat_column].astype('category')\n",
    "            \n",
    "    cats_date = cat_columns + ['date_block_num']\n",
    "    df_cat = df_all_data[cats_date]\n",
    "    df_scale = df_all_data[list(set(df_all_data.columns) - set(cat_columns))]\n",
    "      \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(trim_12(df_scale).drop(columns=['target']))\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(trim_12(df_cat)[cat_columns])\n",
    "    \n",
    "    train_cat, X_test_cat = divide_df(df_cat, TRAIN_SIZE)\n",
    "    train_cat = trim_12(train_cat)\n",
    "    X_test_cat = trim_12(X_test_cat).drop(columns=['date_block_num'])\n",
    "    \n",
    "    max_date = train_cat['date_block_num'].max()\n",
    "               \n",
    "    cv_X_test_cat = train_cat[train_cat['date_block_num'] == max_date].drop(columns=['date_block_num'])\n",
    "    cv_X_train_cat = train_cat[train_cat['date_block_num'] < max_date].drop(columns=['date_block_num'])\n",
    "    \n",
    "    train_scale, X_test_scale = divide_df(df_scale, TRAIN_SIZE)\n",
    "    train_scale = trim_12(train_scale)\n",
    "    X_test_scale = trim_12(X_test_scale)\n",
    "    X_train_scale = train_scale.drop(columns=['target'])\n",
    "    Y_train = train_scale['target']\n",
    "    \n",
    "    cv_X_test_scale = X_train_scale[X_train_scale['date_block_num'] == max_date]\n",
    "    cv_Y_test = Y_train[X_train_scale['date_block_num'] == max_date]\n",
    "\n",
    "    cv_Y_train = Y_train[X_train_scale['date_block_num'] < max_date]\n",
    "    cv_X_train_scale = X_train_scale[X_train_scale['date_block_num'] < max_date]\n",
    "               \n",
    "    # scaling\n",
    "    X_test_scale = scaler.transform(X_test_scale)\n",
    "    cv_X_test_scale = scaler.transform(cv_X_test_scale)\n",
    "    cv_X_train_scale = scaler.transform(cv_X_train_scale)\n",
    "    \n",
    "    X_test_cat = encoder.transform(X_test_cat)\n",
    "    X_test_scale = csr_matrix(X_test_scale)\n",
    "    X_test = hstack((X_test_cat, X_test_scale))\n",
    "               \n",
    "    cv_X_test_cat = encoder.transform(cv_X_test_cat)\n",
    "    cv_X_test_scale = csr_matrix(cv_X_test_scale)\n",
    "    cv_X_test = hstack((cv_X_test_cat, cv_X_test_scale))\n",
    "    \n",
    "    cv_X_train_cat = encoder.transform(cv_X_train_cat)    \n",
    "    cv_X_train_scale = csr_matrix(cv_X_train_scale)\n",
    "    cv_X_train = hstack((cv_X_train_cat, cv_X_train_scale))\n",
    "               \n",
    "    return cv_X_train, cv_Y_train, cv_X_test, cv_Y_test,X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, encode_target_by_category(sales, 'shop_id'), \n",
    "                        how='left', \n",
    "                        on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "def encode_target_by_category(df, cat_column):\n",
    "    df = df[[cat_column, 'date_block_num', 'item_cnt_day']]\n",
    "    ind_cols = [cat_column, 'date_block_num']\n",
    "    summ = df.groupby(ind_cols, as_index=False)['item_cnt_day'].sum()\n",
    "    print(summ)\n",
    "    summ[cat_column + '_target_enc'] = summ.groupby([cat_column], as_index=False)\\\n",
    "        .apply(lambda  x: x.sort_values(by='date_block_num')['item_cnt_day'].cumsum()).reset_index(drop=True)\n",
    "    summ['cumcount'] = summ.groupby(cat_column).cumcount()\n",
    "    \n",
    "    print(summ)\n",
    "    \n",
    "    summ[cat_column + '_target_enc'] = summ[cat_column + '_target_enc'] - summ['item_cnt_day']\n",
    "    summ[cat_column + '_target_enc'] = summ[cat_column + '_target_enc'] / summ['cumcount']\n",
    "    print(summ.head(50))\n",
    "    return summ.drop(columns = ['item_cnt_day', 'cumcount'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
